{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating MMIDAS - Mouse smartseq data\n",
    "This notebook guides you in evaluating MMIDAS in identifying meaningful cell types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import subprocess\n",
    "import os\n",
    "from os import path\n",
    "import glob\n",
    "import functools\n",
    "from typing import Mapping, Any, Literal, Iterable, Sequence, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch import cuda\n",
    "from torch.utils.data import DataLoader\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from sklearn.metrics.cluster import adjusted_mutual_info_score\n",
    "from sklearn.preprocessing import normalize\n",
    "from tqdm import trange, tqdm\n",
    "\n",
    "\n",
    "def run_cmd(cmd: str) -> str:\n",
    "    return subprocess.check_output(cmd, shell=True).decode().strip()\n",
    "\n",
    "\n",
    "def get_parentdirname(d: str) -> str:\n",
    "    return path.basename(path.dirname(d))\n",
    "\n",
    "\n",
    "def is_childdir(d: str, c: str) -> bool:\n",
    "    return c in os.listdir(d)\n",
    "\n",
    "\n",
    "def fix_dir(d: str) -> None:\n",
    "    if os.getcwd().endswith(d):\n",
    "        pass\n",
    "    elif get_parentdirname(os.getcwd()) == d:\n",
    "        os.chdir(\"..\")\n",
    "    elif is_childdir(os.getcwd(), d):\n",
    "        os.chdir(d)\n",
    "\n",
    "\n",
    "def dedup(xs: Iterable) -> Iterable:\n",
    "    acc = []\n",
    "    seen = set()\n",
    "    for x in xs:\n",
    "        if x not in seen:\n",
    "            acc.append(x)\n",
    "        seen.add(x)\n",
    "    return acc\n",
    "\n",
    "\n",
    "def tree_reload() -> None:\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "def unstable(func):\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        warnings.warn(\n",
    "            f\"{func.__name__}() is unstable\", category=FutureWarning, stacklevel=2\n",
    "        )\n",
    "        return func(*args, **kwargs)\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "DIR = \"distributed-vae\"  # warning: change this if you change the name of the directory\n",
    "\n",
    "fix_dir(DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mmidas\n",
    "from mmidas.nn_model import mixVAE_model\n",
    "from mmidas.utils.tree_based_analysis import get_merged_types\n",
    "from mmidas.cpl_mixvae import cpl_mixVAE, unwrap, to_np\n",
    "from mmidas.utils.tools import get_paths\n",
    "from mmidas.utils.dataloader import load_data, get_loaders\n",
    "from mmidas.utils.cluster_analysis import K_selection\n",
    "from mmidas.eval_models import summarize_inference\n",
    "from mmidas.model import generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MouseSmartSeq = Literal[\"mouse_smartseq\"]\n",
    "Mouse10x = Literal[\"TODO\"]\n",
    "SeattleAlzheimer = Literal[\"TODO\"]\n",
    "\n",
    "Dataset = MouseSmartSeq | Mouse10x | SeattleAlzheimer\n",
    "\n",
    "dataset = \"mouse_smartseq\"\n",
    "config: Mapping[str, Any] = get_paths(\"mmidas.toml\", dataset)\n",
    "data: Mapping[str, Any] = load_data(\n",
    "    config[dataset][\"data_path\"] / config[dataset][\"anndata_file\"]\n",
    ")\n",
    "dedup(type(v) for v in data.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trained_models(f: str) -> Sequence[str]:\n",
    "    return glob.glob(f + \"/model/cpl_mixVAE_model_before**\")\n",
    "\n",
    "\n",
    "trained_model_folder = config[dataset][\"trained_model\"]\n",
    "saving_folder = str(\n",
    "    config[\"paths\"][\"main_dir\"] / config[dataset][\"saving_path\"] / trained_model_folder\n",
    ")\n",
    "trained_models = get_trained_models(saving_folder)\n",
    "selected_model = trained_models[0]\n",
    "selected_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Construct a cpl-mixVAE object and load the trained_model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: just parse the file name to get these params\n",
    "\n",
    "n_categories = 92  # upper bound of number of categories (clusters)\n",
    "state_dim = 2  # continuous (state) variable dimensionality\n",
    "n_arm = 3  # number of arms\n",
    "latent_dim = 10  # latent dimensionality of the model\n",
    "train_loader, test_loader, all_loader = get_loaders(\n",
    "    dataset=data[\"log1p\"], batch_size=5000, seed=546\n",
    ")\n",
    "\n",
    "# cplMixVAE = cpl_mixVAE(saving_folder=saving_folder, device='cpu')\n",
    "# cplMixVAE.init_model(n_categories=n_categories,\n",
    "#                      state_dim=state_dim,\n",
    "#                      input_dim=data['log1p'].shape[1],\n",
    "#                      lowD_dim=latent_dim,\n",
    "#                      n_arm=n_arm)\n",
    "# cplMixVAE.variational = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = cpl_mixVAE(saving_folder=saving_folder, device=\"cuda\")\n",
    "trainer.init_model(\n",
    "    n_categories=n_categories,\n",
    "    state_dim=state_dim,\n",
    "    input_dim=data[\"log1p\"].shape[1],\n",
    "    lowD_dim=latent_dim,\n",
    "    n_arm=n_arm,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection\n",
    "\n",
    "##### Determining the sub-optimal number of clusters by leveraging consensus values across multiple pruning runs.\n",
    "If you do not employ the pruning algorithm to determine the number of clusters, you can skip the following step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading trained models including before pruning and after pruning\n",
    "summary = summarize_inference(cplMixVAE, trained_models, train_loader)\n",
    "summary.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting average consensus measure to select the number of clusters according to the minimum consensus measure, here 0.95\n",
    "\n",
    "summary_file = saving_folder + f\"/summary_performance_K_{n_categories}_narm_{n_arm}.p\"\n",
    "with open(summary_file, \"rb\") as f:\n",
    "    summary_dict = np.load(f, allow_pickle=True)\n",
    "f.close()\n",
    "\n",
    "model_order = K_selection(summary_dict, n_categories, n_arm, thr=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assessing the performance of the model for the chosen model order, which denotes the dimensionality of the categorical representation in the mixture VAE model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_model = glob.glob(saving_folder + '/model/cpl_mixVAE_model_after_pruning_' + str(n_categories - model_order) + '*')[0]\n",
    "model_order = n_categories\n",
    "selected_model = glob.glob(saving_folder + \"/model/cpl_mixVAE_model_before_**\")[0]\n",
    "outcome = summarize_inference(cplMixVAE, selected_model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Run = str | int\n",
    "Config = Mapping[str, Any]\n",
    "TrainedModelFile = str\n",
    "Summary = Mapping[Any, Any]\n",
    "Arm = int\n",
    "Arms = int\n",
    "\n",
    "Consensus = float\n",
    "MI = float\n",
    "\n",
    "TrainedModel = nn.Module\n",
    "\n",
    "Bias = th.Tensor\n",
    "PruningMask = th.Tensor | np.ndarray\n",
    "PruningIndex = th.Tensor\n",
    "\n",
    "MixVAE = mixVAE_model\n",
    "\n",
    "\n",
    "def mk_vae(\n",
    "    C,\n",
    "    state_dim,\n",
    "    input_dim,\n",
    "    device,\n",
    "    eps=1e-8,\n",
    "    fc_dim=100,\n",
    "    latent_dim=10,\n",
    "    x_drop=0.5,\n",
    "    s_drop=0.2,\n",
    "    lr=0.001,\n",
    "    lam=1,\n",
    "    lam_pc=1,\n",
    "    A=2,\n",
    "    tau=0.005,\n",
    "    beta=1.0,\n",
    "    hard=False,\n",
    "    variational=True,\n",
    "    ref_prior=False,\n",
    "    momentum=0.01,\n",
    "    mode=\"MSE\",\n",
    ") -> nn.Module:\n",
    "    return mixVAE_model(\n",
    "        input_dim=input_dim,\n",
    "        fc_dim=fc_dim,\n",
    "        n_categories=C,\n",
    "        state_dim=state_dim,\n",
    "        lowD_dim=latent_dim,\n",
    "        x_drop=x_drop,\n",
    "        s_drop=s_drop,\n",
    "        n_arm=A,\n",
    "        lam=lam,\n",
    "        lam_pc=lam_pc,\n",
    "        tau=tau,\n",
    "        beta=beta,\n",
    "        hard=hard,\n",
    "        variational=variational,\n",
    "        device=device,\n",
    "        eps=eps,\n",
    "        ref_prior=ref_prior,\n",
    "        momentum=momentum,\n",
    "        loss_mode=mode,\n",
    "    ).to(device)\n",
    "\n",
    "\n",
    "def _mk_vae_cfg(A: int) -> Mapping[str, Any]:\n",
    "    return {\n",
    "        \"C\": 92,\n",
    "        \"state_dim\": 2,\n",
    "        \"input_dim\": data[\"log1p\"].shape[1],\n",
    "        \"device\": \"cuda\",\n",
    "        \"A\": A,\n",
    "        \"latent_dim\": 10,\n",
    "}\n",
    "\n",
    "\n",
    "def unwrap_literal(x: Dataset) -> str:\n",
    "    return x.__args__[0]\n",
    "\n",
    "\n",
    "def mk_run(arms: int, run: int, epochs: int = 100000) -> Run:\n",
    "    if arms == 5:\n",
    "        return f\"K92_S2_AUGTrue_LR0.001_A{arms}_B5000_E200000_Ep0_RUN{run}\"\n",
    "    else:\n",
    "        return f\"K92_S2_AUGTrue_LR0.001_A{arms}_B5000_E{epochs}_Ep0_RUN{run}\"\n",
    "\n",
    "\n",
    "def mk_config(r: Run, d: Dataset) -> Config:\n",
    "    config = get_paths(\"mmidas.toml\", unwrap_literal(d))\n",
    "    config[\"mouse_smartseq\"][\"trained_model\"] = r\n",
    "    return config\n",
    "\n",
    "\n",
    "def get_weights(r: Run, d: Dataset) -> TrainedModelFile:\n",
    "    c: Config = mk_config(r, d)\n",
    "    saving_folder = c[\"paths\"][\"main_dir\"] / c[unwrap_literal(d)][\"saving_path\"]\n",
    "    trained_model_folder = c[unwrap_literal(d)][\"trained_model\"]\n",
    "    saving_folder = str(saving_folder / trained_model_folder)\n",
    "    trained_models = glob.glob(saving_folder + \"/model/cpl_mixVAE_model_before**\")\n",
    "    assert len(trained_models) == 1\n",
    "    return trained_models[0]\n",
    "\n",
    "def view_weights(arms: int, run: int) -> None:\n",
    "    return th.load(get_weights(mk_run(arms, run), MouseSmartSeq), map_location=\"cpu\")[\"model_state_dict\"]\n",
    "\n",
    "\n",
    "def load_weights(m: nn.Module, f: str) -> None:\n",
    "    m.load_state_dict(th.load(f, map_location=\"cpu\")[\"model_state_dict\"])\n",
    "\n",
    "\n",
    "def load_vae(arms: int, run: int) -> MixVAE:\n",
    "    r = mk_run(arms, run)\n",
    "    vae = mk_vae(**_mk_vae_cfg(arms))\n",
    "    print(get_weights(r, MouseSmartSeq))\n",
    "    load_weights(vae, get_weights(r, MouseSmartSeq))\n",
    "    return vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@unstable\n",
    "def evals(f: nn.Module, dl: DataLoader) -> Mapping[str, Any]:\n",
    "    C = f.n_categories\n",
    "    outs = generate(f, dl)\n",
    "\n",
    "    preds = outs[\"preds\"]\n",
    "    inds_prune = outs[\"inds_prune\"]\n",
    "    pm = []\n",
    "    emp_l2 = []\n",
    "    emp_log = []\n",
    "    consensus = []\n",
    "    consensus_min = []\n",
    "    consensus_mean = []\n",
    "    dist_l2 = []\n",
    "    dist_log = []\n",
    "    for a, pred_a in tqdm(enumerate(preds)):\n",
    "        for pred_b in preds[a + 1]:\n",
    "            _pm = np.zeros((C, C))  # performance matrix for arm a vs arm b\n",
    "            _emp_l2 = np.zeros((C, C)) # empirical matrix for arm a vs arm b\n",
    "            _emp_log = np.zeros((C, C)) # empirical matrix for arm a vs arm b\n",
    "            for samp_a, samp_b in zip(pred_a, pred_b):\n",
    "                _pm[samp_a.astype(int) - 1, samp_b.astype(int) - 1] += 1\n",
    "                _emp_l2[samp_a.astype(int) - 1, samp_b.astype(int) - 1] += np.sqrt(np.sum((samp_a - samp_b) ** 2))\n",
    "                _emp_log[samp_a.astype(int) - 1, samp_b.astype(int) - 1] += 0.5 * (samp_a * np.log(samp_a / samp_b) + samp_b * np.log(samp_b / samp_a))\n",
    "\n",
    "\n",
    "            smp_cts = []\n",
    "            for c in range(C):\n",
    "                smp_cts.append(max(_pm[c].sum(), _pm[:, c].sum()))\n",
    "            smp_cts = np.array(smp_cts)\n",
    "\n",
    "            inds_unpruned = np.where(np.isin(range(C), inds_prune) == False)[0]\n",
    "            _consensus = np.divide(\n",
    "                _pm, smp_cts, out=np.zeros_like(_pm), where=smp_cts != 0\n",
    "            )[:, inds_unpruned][inds_unpruned]  # check this\n",
    "            _dist_l2 = np.divide(\n",
    "                _emp_l2, smp_cts, out=np.zeros_like(_emp_l2), where=smp_cts != 0\n",
    "            )[:, inds_unpruned][inds_unpruned]\n",
    "            _dist_log = np.divide(\n",
    "                _emp_log, smp_cts, out=np.zeros_like(_emp_log), where=smp_cts != 0\n",
    "            )[:, inds_unpruned][inds_unpruned]\n",
    "\n",
    "            consensus.append(_consensus)\n",
    "            consensus_min.append(np.min(np.diag(_consensus)))\n",
    "            consensus_mean.append(\n",
    "                1.0 - ((np.abs(preds[0] - preds[1]) > 0.0).sum() / preds.shape[1])\n",
    "            )\n",
    "            dist_l2.append(_dist_l2)\n",
    "            dist_log.append(_dist_log)\n",
    "            pm.append(_pm[inds_unpruned][:, inds_unpruned])\n",
    "            emp_l2.append(_emp_l2[inds_unpruned][:, inds_unpruned])\n",
    "            emp_log.append(_emp_log[inds_unpruned][:, inds_unpruned])\n",
    "\n",
    "    return {\n",
    "        \"c_dists\": outs[\"c_dists\"],\n",
    "        \"c_l2_dists\": outs[\"c_l2_dists\"],\n",
    "        \"preds\": outs[\"preds\"],\n",
    "        \"inds_prune\": outs[\"inds_prune\"],\n",
    "        \"s_means\": outs[\"s_means\"],\n",
    "        \"s_logvars\": outs[\"s_logvars\"],\n",
    "        \"inds_x\": outs[\"inds_x\"],\n",
    "        \"cs\": outs[\"cs\"],\n",
    "        \"x_recs\": outs[\"x_recs\"],\n",
    "        \"x_lows\": outs[\"x_lows\"],\n",
    "        \"consensus\": consensus,\n",
    "        \"consensus_min\": consensus_min,\n",
    "        \"consensus_mean\": consensus_mean,\n",
    "        \"pm\": pm,\n",
    "        \"inds_unpruned\": inds_unpruned,\n",
    "        \"emp_l2\": emp_l2,\n",
    "        \"emp_log\": emp_log,\n",
    "        \"dist_l2\": dist_l2,\n",
    "        \"dist_log\": dist_log,\n",
    "    }\n",
    "\n",
    "\n",
    "def evals2(fa: nn.Module, fb: nn.Module, dl: DataLoader, eps=1e-9) -> Mapping[str, Any]:\n",
    "    C = fa.n_categories\n",
    "    outs_a = generate(fa, dl)\n",
    "    outs_b = generate(fb, dl)\n",
    "\n",
    "    preds_a = outs_a[\"preds\"]\n",
    "    preds_b = outs_b[\"preds\"]\n",
    "    inds_prune = outs_a[\"inds_prune\"]\n",
    "\n",
    "    qcas = outs_a[\"cs\"]\n",
    "    qcbs = outs_b[\"cs\"]\n",
    "\n",
    "    consensus = []\n",
    "    consensus_min = []\n",
    "    consensus_mean = []\n",
    "    dist_l2 = []\n",
    "    dist_log = []\n",
    "    pm = []\n",
    "    emp_l2 = []\n",
    "    emp_log = []\n",
    "\n",
    "    consensus_a = []\n",
    "    consensus_min_a = []\n",
    "    consensus_mean_a = []\n",
    "    pm_a = []\n",
    "    dist_l2_a = []\n",
    "    dist_log_a = []\n",
    "    emp_l2_a = []\n",
    "    emp_log_a = []\n",
    "\n",
    "    consensus_b = []\n",
    "    consensus_min_b = []\n",
    "    consensus_mean_b = []\n",
    "    pm_b = []\n",
    "    dist_l2_b = []\n",
    "    dist_log_b = []\n",
    "    emp_l2_b = []\n",
    "    emp_log_b = []\n",
    "    for (a, pred_a) in tqdm(enumerate(preds_a), total=len(preds_a)):\n",
    "        for (b, pred_b) in enumerate(preds_b):\n",
    "            _pm = np.zeros((C, C))  # performance matrix for arm a vs arm b\n",
    "            _emp_l2 = np.zeros((C, C)) # empirical matrix for arm a vs arm b\n",
    "            _emp_log = np.zeros((C, C)) # empirical matrix for arm a vs arm b\n",
    "            for (samp_a, samp_b, qca, qcb) in zip(pred_a, pred_b, qcas[a], qcbs[b]):\n",
    "                i_a = samp_a.astype(int) - 1\n",
    "                i_b = samp_b.astype(int) - 1\n",
    "                _pm[i_a, i_b] += 1\n",
    "                _emp_l2[i_a, i_b] += np.sqrt((qca[i_a]- qcb[i_b]) ** 2)\n",
    "                _emp_log[i_a, i_b] += 0.5 * (qca[i_a] * np.log(qca[i_a] / (qcb[i_b] + eps)) + qcb[i_b] * np.log(qcb[i_b] / (qca[i_a] + eps)))\n",
    "                \n",
    "            smp_cts = []\n",
    "            for c in range(C):\n",
    "                print(\"_pm[c]\", _pm[c])\n",
    "                print(\"_pm[:, c]\", _pm[:, c])\n",
    "                smp_cts.append(max(_pm[c].sum(), _pm[:, c].sum()))\n",
    "            smp_cts = np.array(smp_cts)\n",
    "\n",
    "            inds_unpruned = np.where(np.isin(range(C), inds_prune) == False)[0]\n",
    "            _consensus = np.divide(\n",
    "                _pm, smp_cts, out=np.zeros_like(_pm), where=smp_cts != 0\n",
    "            )[:, inds_unpruned][inds_unpruned]\n",
    "            _dist_l2 = np.divide(\n",
    "                _emp_l2, smp_cts, out=np.zeros_like(_emp_l2), where=smp_cts != 0\n",
    "            )[:, inds_unpruned][inds_unpruned]\n",
    "            _dist_log = np.divide(\n",
    "                _emp_log, smp_cts, out=np.zeros_like(_emp_log), where=smp_cts != 0\n",
    "            )[:, inds_unpruned][inds_unpruned]\n",
    "\n",
    "            consensus.append(_consensus)\n",
    "            consensus_min.append(np.min(np.diag(_consensus)))\n",
    "            consensus_mean.append(\n",
    "                1.0 - ((np.abs(preds_a[0] - preds_b[0]) > 0.0).sum() / preds_a.shape[1])\n",
    "            )\n",
    "            pm.append(_pm[inds_unpruned][:, inds_unpruned])\n",
    "            dist_l2.append(_dist_l2)\n",
    "            dist_log.append(_dist_log)\n",
    "            pm.append(_pm[inds_unpruned][:, inds_unpruned])\n",
    "            emp_l2.append(_emp_l2[inds_unpruned][:, inds_unpruned])\n",
    "            emp_log.append(_emp_log[inds_unpruned][:, inds_unpruned])\n",
    "\n",
    "        for (b, pred_b) in enumerate(preds_a[a + 1 :]):\n",
    "            _pm = np.zeros((C, C))\n",
    "            _emp_l2 = np.zeros((C, C))\n",
    "            _emp_log = np.zeros((C, C))\n",
    "            for (samp_a, samp_b, qca, qcb) in zip(pred_a, pred_b, qcas[a], qcas[b]):\n",
    "                i_a = samp_a.astype(int) - 1\n",
    "                i_b = samp_b.astype(int) - 1\n",
    "                _pm[i_a, i_b] += 1\n",
    "                _emp_l2[i_a, i_b] += np.sqrt((qca[i_a]- qcb[i_b]) ** 2)\n",
    "                _emp_log[i_a, i_b] += 0.5 * (qca[i_a] * np.log(qca[i_a] / (qcb[i_b] + eps)) + qcb[i_b] * np.log(qcb[i_b] / (qca[i_a] + eps)))\n",
    "\n",
    "            smp_cts = []\n",
    "            for c in range(C):\n",
    "                smp_cts.append(max(_pm[c].sum(), _pm[:, c].sum()))\n",
    "            smp_cts = np.array(smp_cts)\n",
    "\n",
    "            inds_unpruned = np.where(np.isin(range(C), inds_prune) == False)[0]\n",
    "            _consensus = np.divide(\n",
    "                _pm, smp_cts, out=np.zeros_like(_pm), where=smp_cts != 0\n",
    "            )[:, inds_unpruned][inds_unpruned]\n",
    "            _dist_l2 = np.divide(\n",
    "                _emp_l2, smp_cts, out=np.zeros_like(_emp_l2), where=smp_cts != 0\n",
    "            )[:, inds_unpruned][inds_unpruned]\n",
    "            _dist_log = np.divide(\n",
    "                _emp_log, smp_cts, out=np.zeros_like(_emp_log), where=smp_cts != 0\n",
    "            )[:, inds_unpruned][inds_unpruned]\n",
    "\n",
    "            consensus_a.append(_consensus)\n",
    "            consensus_min_a.append(np.min(np.diag(_consensus)))\n",
    "            consensus_mean_a.append(\n",
    "                1.0 - ((np.abs(preds_a[0] - preds_a[1]) > 0.0).sum() / preds_a.shape[1])\n",
    "            )\n",
    "            pm_a.append(_pm[inds_unpruned][:, inds_unpruned])\n",
    "            dist_l2_a.append(_dist_l2)\n",
    "            dist_log_a.append(_dist_log)\n",
    "            emp_l2_a.append(_emp_l2[inds_unpruned][:, inds_unpruned])\n",
    "            emp_log_a.append(_emp_log[inds_unpruned][:, inds_unpruned])\n",
    "\n",
    "    for a, pred_a in tqdm(enumerate(preds_b), total=len(preds_b)):\n",
    "        for (b, pred_b) in enumerate(preds_b[a + 1 :]):\n",
    "            _pm = np.zeros((C, C))\n",
    "            _emp_l2 = np.zeros((C, C))\n",
    "            _emp_log = np.zeros((C, C))\n",
    "            for (samp_a, samp_b, qca, qcb) in zip(pred_a, pred_b, qcbs[a], qcbs[b]):\n",
    "                i_a = samp_a.astype(int) - 1\n",
    "                i_b = samp_b.astype(int) - 1\n",
    "                _pm[i_a, i_b] += 1\n",
    "                _emp_l2[i_a, i_b] += np.sqrt((qca[i_a]- qcb[i_b]) ** 2)\n",
    "                _emp_log[i_a, i_b] += 0.5 * (qca[i_a] * np.log(qca[i_a] / (qcb[i_b] + eps)) + qcb[i_b] * np.log(qcb[i_b] / (qca[i_a] + eps)))\n",
    "\n",
    "            smp_cts = []\n",
    "            for c in range(C):\n",
    "                smp_cts.append(max(_pm[c].sum(), _pm[:, c].sum()))\n",
    "            smp_cts = np.array(smp_cts)\n",
    "\n",
    "            inds_unpruned = np.where(np.isin(range(C), inds_prune) == False)[0]\n",
    "            _consensus = np.divide(\n",
    "                _pm, smp_cts, out=np.zeros_like(_pm), where=smp_cts != 0\n",
    "            )[:, inds_unpruned][inds_unpruned]\n",
    "            _dist_l2 = np.divide(\n",
    "                _emp_l2, smp_cts, out=np.zeros_like(_emp_l2), where=smp_cts != 0\n",
    "            )[:, inds_unpruned][inds_unpruned]\n",
    "            _dist_log = np.divide(\n",
    "                _emp_log, smp_cts, out=np.zeros_like(_emp_log), where=smp_cts != 0\n",
    "            )[:, inds_unpruned][inds_unpruned]\n",
    "\n",
    "            consensus_b.append(_consensus)\n",
    "            consensus_min_b.append(np.min(np.diag(_consensus)))\n",
    "            consensus_mean_b.append(\n",
    "                1.0 - ((np.abs(preds_b[0] - preds_b[1]) > 0.0).sum() / preds_b.shape[1])\n",
    "            )\n",
    "            pm_b.append(_pm[inds_unpruned][:, inds_unpruned])\n",
    "            dist_l2_b.append(_dist_l2)\n",
    "            dist_log_b.append(_dist_log)\n",
    "            emp_l2_b.append(_emp_l2[inds_unpruned][:, inds_unpruned])\n",
    "            emp_log_b.append(_emp_log[inds_unpruned][:, inds_unpruned])\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"consensus\": consensus,\n",
    "        \"consensus_min\": consensus_min,\n",
    "        \"consensus_mean\": consensus_mean,\n",
    "        \"pm\": pm,\n",
    "        \"consensus_a\": consensus_a,\n",
    "        \"consensus_min_a\": consensus_min_a,\n",
    "        \"consensus_mean_a\": consensus_mean_a,\n",
    "        \"pm_a\": pm_a,\n",
    "        \"consensus_b\": consensus_b,\n",
    "        \"consensus_min_b\": consensus_min_b,\n",
    "        \"consensus_mean_b\": consensus_mean_b,\n",
    "        \"pm_b\": pm_b,\n",
    "        \"inds_unpruned\": inds_unpruned,\n",
    "        \"cs_a\": outs_a[\"cs\"],\n",
    "        \"cs_b\": outs_b[\"cs\"],\n",
    "        \"dist_l2\": dist_l2,\n",
    "        \"dist_log\": dist_log,\n",
    "        \"emp_l2\": emp_l2,\n",
    "        \"emp_log\": emp_log,\n",
    "        \"dist_l2_a\": dist_l2_a,\n",
    "        \"dist_log_a\": dist_log_a,\n",
    "        \"emp_l2_a\": emp_l2_a,\n",
    "        \"emp_log_a\": emp_log_a,\n",
    "        \"dist_l2_b\": dist_l2_b,\n",
    "        \"dist_log_b\": dist_log_b,\n",
    "        \"emp_l2_b\": emp_l2_b,\n",
    "    }\n",
    "\n",
    "\n",
    "def compare_state_dicts(model1: nn.Module, model2: nn.Module, rtol=1e-5, atol=1e-8):\n",
    "    state_dict1 = model1.state_dict()\n",
    "    state_dict2 = model2.state_dict()\n",
    "\n",
    "    if state_dict1.keys() != state_dict2.keys():\n",
    "        print(\"The state dictionaries have different keys.\")\n",
    "        return False\n",
    "\n",
    "    for key in state_dict1.keys():\n",
    "        if not th.allclose(state_dict1[key], state_dict2[key], rtol=rtol, atol=atol):\n",
    "            print(f\"Mismatch found in layer: {key}\")\n",
    "            return False\n",
    "\n",
    "    print(\"The state dictionaries are identical within the specified tolerance.\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = 2\n",
    "B = 2\n",
    "ra = 0\n",
    "rb = 1\n",
    "vae_a = load_vae(A, ra)\n",
    "vae_b = load_vae(B, rb)\n",
    "\n",
    "ev = evals2(vae_a, vae_b, all_loader)\n",
    "len(ev[\"consensus\"]), len(ev[\"consensus_a\"]), len(ev[\"consensus_b\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_axis(axes: tuple[Arm, Arms, Optional[Run]]) -> str:\n",
    "    if axes[0] == \"ttypes\" and axes[1] == \"ttypes\":\n",
    "        return \"Ttypes classification\"\n",
    "    elif axes[2] is not None:\n",
    "        return f\"Arm {axes[0]} of {axes[1]}-arm MMIDAS, run {axes[2]}\"\n",
    "    else:\n",
    "        return f\"Arm {axes[0]} of {axes[1]}-arm MMIDAS\"\n",
    "\n",
    "\n",
    "def get_axis_save(axes: tuple[Arm, Arms, Optional[Run]]) -> str:\n",
    "    if axes[0] == \"ttypes\" and axes[1] == \"ttypes\":\n",
    "        return \"tt\"\n",
    "    elif axes[2] is not None:\n",
    "        return f\"{axes[0]}{axes[1]}{axes[2]}\"\n",
    "    else:\n",
    "        return f\"{axes[0]}{axes[1]}\"\n",
    "\n",
    "\n",
    "def plot_consensus_circ(\n",
    "    a_vs_b: np.ndarray,\n",
    "    axes: tuple[tuple[Arms, Arm, Optional[Run]], tuple[Arms, Arm, Optional[Run]]],\n",
    "    savedir: Optional[str],\n",
    ") -> None:\n",
    "    assert a_vs_b.shape[0] == a_vs_b.shape[1]\n",
    "\n",
    "    mtx = a_vs_b / np.max(a_vs_b)\n",
    "    C = mtx.shape[0]  # number of clusters\n",
    "\n",
    "    fig, axs = plt.subplots(1, 1, figsize=(10, 10))\n",
    "    for l in trange(C):\n",
    "        for col in range(C):\n",
    "            axs.add_patch(\n",
    "                plt.Circle(np.array([col, l]), radius=mtx[l, col], color=\"Navy\")\n",
    "            )\n",
    "    axs.set_xlim([-0.5, C])\n",
    "    axs.set_ylim([-0.5, C - 0.5])\n",
    "    axs.invert_yaxis()\n",
    "    axs.set_yticks([])\n",
    "    axs.set_xticks([])\n",
    "    axs.set_yticklabels([])\n",
    "    axs.set_xticklabels([])\n",
    "    plt.title(\"Consensus for |c|= \" + str(a_vs_b.shape[0]), fontsize=24)\n",
    "    axs.set_xlabel(get_axis(axes[0]), fontsize=20)\n",
    "    axs.set_ylabel(get_axis(axes[1]), fontsize=20)\n",
    "    fig.tight_layout()\n",
    "    if savedir:\n",
    "        plt.savefig(\n",
    "            savedir\n",
    "            + f\"/consensus_circ_{get_axis_save(axes[0])}_vs_{get_axis_save(axes[1])}_K{C}.png\",\n",
    "            dpi=600,\n",
    "        )\n",
    "\n",
    "\n",
    "so_far = 0\n",
    "so_far_a = 0\n",
    "so_far_b = 0\n",
    "for a in range(A):\n",
    "    for b in range(B):\n",
    "        plot_consensus_circ(\n",
    "            ev[\"pm\"][so_far], ((a, A, ra), (b, B, rb)), \n",
    "            # None\n",
    "            f\"multiarm-results/{A}{B}\"\n",
    "        )\n",
    "        so_far += 1\n",
    "\n",
    "    for b in range(a + 1, A):\n",
    "        plot_consensus_circ(\n",
    "            ev[\"pm_a\"][so_far_a],\n",
    "            ((a, A, ra), (b, A, ra)),\n",
    "            # None\n",
    "            f\"multiarm-results/{A}{B}\",\n",
    "        )\n",
    "        so_far_a += 1\n",
    "\n",
    "for a in range(B):\n",
    "    for b in range(a + 1, B):\n",
    "        plot_consensus_circ(\n",
    "            ev[\"pm_b\"][so_far_b],\n",
    "            ((a, B, rb), (b, B, rb)),\n",
    "            f\"multiarm-results/{A}{B}\",\n",
    "            # None,\n",
    "        )\n",
    "        so_far_b += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev[\"consensus\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConsensusMatrix = np.ndarray\n",
    "\n",
    "def matched_mean(x):\n",
    "    _, col_inds = linear_sum_assignment(-x)\n",
    "    x = x[:, col_inds]\n",
    "    return np.mean(np.diag(x))\n",
    "\n",
    "def plot_consensus_heatmap(\n",
    "    cm: ConsensusMatrix,\n",
    "    axes: tuple[tuple[Arm, Arms, Optional[Run]], tuple[Arm, Arms, Optional[Run]]],\n",
    "    savedir: Optional[str],\n",
    "    between_runs,\n",
    "    same_runs,\n",
    ") -> None:\n",
    "    (arma, armsa, runa), (armb, armsb, runb) = axes\n",
    "    assert armsa == armsb\n",
    "\n",
    "    _, col_inds = linear_sum_assignment(-cm)\n",
    "    cm = cm[:, col_inds]\n",
    "\n",
    "    plt.figure(figsize=[10, 10])\n",
    "    ax = plt.gca()\n",
    "    im = ax.imshow(cm, cmap=\"rocket\", vmin=0, vmax=1)\n",
    "    plt.xlabel(get_axis(axes[0]), fontsize=30, labelpad=15)\n",
    "    plt.ylabel(get_axis(axes[1]), fontsize=30, labelpad=15)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(\n",
    "        f\"Consenus for |c|= {str(cm.shape[0])} (avg={np.mean(np.diag(cm)):.6f})\",\n",
    "        fontsize=32,\n",
    "        pad=10,\n",
    "    )\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.3)\n",
    "    cbar = plt.colorbar(im, cax=cax)\n",
    "    for t in cbar.ax.get_yticklabels():\n",
    "        t.set_fontsize(20)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if savedir:\n",
    "        plt.savefig(\n",
    "            savedir\n",
    "            + f\"/consensus_heat_{get_axis_save(axes[0])}_vs_{get_axis_save(axes[1])}_K{cm.shape[0]}.png\",\n",
    "            dpi=600,\n",
    "        )\n",
    "    avg_cons = np.mean(np.diag(cm))\n",
    "    if runa == runb:\n",
    "        same_runs.append(avg_cons)\n",
    "    else:\n",
    "        between_runs.append(avg_cons)\n",
    "    print(f\"avg consensus on test cells: {avg_cons:.6f} (arm {arma}, run {runa}) vs (arm {armb}, run {runb})\")\n",
    "\n",
    "\n",
    "\n",
    "so_far = 0\n",
    "so_far_a = 0\n",
    "so_far_b = 0\n",
    "between_runs = []\n",
    "same_runs = []\n",
    "between_runs_l2 = []\n",
    "same_runs_l2 = []\n",
    "between_runs_log = []\n",
    "same_runs_log = []\n",
    "\n",
    "for a in range(A):\n",
    "    for b in range(B):\n",
    "        plot_consensus_heatmap(\n",
    "            ev[\"consensus\"][so_far],\n",
    "            ((a, A, ra), (b, B, rb)),\n",
    "            f\"multiarm-results/{A}{B}\",\n",
    "            between_runs,\n",
    "            same_runs\n",
    "            # None\n",
    "        )\n",
    "        between_runs_l2.append(matched_mean(ev['dist_l2'][so_far]))\n",
    "        between_runs_log.append(matched_mean(ev['dist_log'][so_far]))\n",
    "        print(f\"average L2 distance: {between_runs_l2[-1]} (arm {a} run {ra}) , (arm {b} run {rb})\")\n",
    "        print(f\"average log distance: {between_runs_log[-1]} (arm {a} run {ra}), (arm {b} run {rb})\")\n",
    "        so_far += 1\n",
    "\n",
    "    for b in range(a + 1, A):\n",
    "        plot_consensus_heatmap(\n",
    "            ev[\"consensus_a\"][so_far_a],\n",
    "            ((a, A, ra), (b, A, ra)),\n",
    "            f\"multiarm-results/{A}{B}\",\n",
    "            between_runs,\n",
    "            same_runs\n",
    "            # None\n",
    "        )\n",
    "        same_runs_l2.append(matched_mean(ev['dist_l2_a'][so_far_a]))\n",
    "        same_runs_log.append(matched_mean(ev['dist_log_a'][so_far_a]))\n",
    "        print(f\"average L2 distance: {same_runs_l2[-1]} (arm {a} run {ra}) , (arm {b} run {ra})\")\n",
    "        print(f\"average log distance: {same_runs_log[-1]} (arm {a} run {ra}), (arm {b} run {ra})\")\n",
    "        so_far_a += 1\n",
    "\n",
    "for a in range(B):\n",
    "    for b in range(a + 1, B):\n",
    "        plot_consensus_heatmap(\n",
    "            ev[\"consensus_b\"][so_far_b],\n",
    "            ((a, B, rb), (b, B, rb)),\n",
    "            f\"multiarm-results/{A}{B}\",\n",
    "            between_runs,\n",
    "            same_runs\n",
    "            # None\n",
    "        )\n",
    "        same_runs_l2.append(matched_mean(ev['dist_l2_b'][so_far_b]))\n",
    "        same_runs_log.append(matched_mean(ev['dist_log_b'][so_far_b]))\n",
    "        print(f\"average L2 distance: {same_runs_l2[-1]} (arm {a} run {rb}) , (arm {b} run {rb})\")\n",
    "        print(f\"average log distance: {same_runs_log[-1]} (arm {a} run {rb}), (arm {b} run {rb})\")\n",
    "        so_far_b += 1\n",
    "\n",
    "print()\n",
    "print(f\"Average consensus between runs: {np.mean(np.array(between_runs))}\")\n",
    "print(f\"Average consensus within runs: {np.mean(np.array(same_runs))}\")\n",
    "print(f\"Ratio of between within runs: {np.mean(np.array(between_runs)) / np.mean(np.array(same_runs))}\")\n",
    "print()\n",
    "print(f\"Average L2 distance between runs: {np.mean(np.array(between_runs_l2))}\")\n",
    "print(f\"Average L2 distance within runs: {np.mean(np.array(same_runs_l2))}\")\n",
    "print(f\"Ratio of between within runs: {np.mean(np.array(between_runs_l2)) / np.mean(np.array(same_runs_l2))}\")\n",
    "print()\n",
    "print(f\"Average log distance between runs: {np.mean(np.array(between_runs_log))}\")\n",
    "print(f\"Average log distance within runs: {np.mean(np.array(same_runs_log))}\")\n",
    "print(f\"Ratio of between within runs: {np.mean(np.array(between_runs_log)) / np.mean(np.array(same_runs_log))}\")\n",
    "\n",
    "# maybe check images the model got wrong to see what features got wrong\n",
    "# how much noise to give?\n",
    "\n",
    "\"\"\"\n",
    "2 arms (0 vs 1)\n",
    "Average consensus between runs: 0.368052575659836\n",
    "Average consensus within runs: 0.6980622817474862\n",
    "Ratio of between within runs: 0.5272489078459817\n",
    "\n",
    "\n",
    "3 arms (0 vs 1)\n",
    "Average consensus between runs: 0.37029876671140843\n",
    "Average consensus within runs: 0.696820754440647\n",
    "Ratio of between to within runs: 0.5314117932791127\n",
    "\n",
    "5 arms (0 vs 1)\n",
    "Average consensus between runs: 0.39533587359290634\n",
    "Average consensus within runs: 0.6682488679328394\n",
    "Ratio of between within runs: 0.591599765542197\n",
    "\n",
    "avg consensus on test cells: 0.359259 (arm 0, run 0) vs (arm 0, run 1)\n",
    "average L2 distance: 0.05313836422523679 (arm 0 run 0) , (arm 0 run 1)\n",
    "average log distance: 0.01301466858369827 (arm 0 run 0), (arm 0 run 1)\n",
    "avg consensus on test cells: 0.356681 (arm 0, run 0) vs (arm 1, run 1)\n",
    "average L2 distance: 0.051257138761952044 (arm 0 run 0) , (arm 1 run 1)\n",
    "average log distance: 0.012636727595115712 (arm 0 run 0), (arm 1 run 1)\n",
    "avg consensus on test cells: 0.713460 (arm 0, run 0) vs (arm 1, run 0)\n",
    "average L2 distance: 0.02356478493766651 (arm 0 run 0) , (arm 1 run 0)\n",
    "average log distance: 0.020833048136219733 (arm 0 run 0), (arm 1 run 0)\n",
    "avg consensus on test cells: 0.354471 (arm 1, run 0) vs (arm 0, run 1)\n",
    "average L2 distance: 0.05354754756987294 (arm 1 run 0) , (arm 0 run 1)\n",
    "average log distance: 0.013091636624093671 (arm 1 run 0), (arm 0 run 1)\n",
    "avg consensus on test cells: 0.359775 (arm 1, run 0) vs (arm 1, run 1)\n",
    "average L2 distance: 0.053289400511152364 (arm 1 run 0) , (arm 1 run 1)\n",
    "average log distance: 0.013224005723399077 (arm 1 run 0), (arm 1 run 1)\n",
    "avg consensus on test cells: 0.687894 (arm 0, run 1) vs (arm 1, run 1)\n",
    "average L2 distance: 0.023574323515396553 (arm 0 run 1) , (arm 1 run 1)\n",
    "average log distance: 0.02452734095868378 (arm 0 run 1), (arm 1 run 1)\n",
    "Average consensus between runs: 0.3575467389489588\n",
    "Average consensus within runs: 0.7006770207865217\n",
    "Ratio of between within runs: 0.5102875195587355\n",
    "\n",
    "Average L2 distance between runs: 0.05280811276705353\n",
    "Average L2 distance within runs: 0.02356955422653153\n",
    "Ratio of between within runs: 2.240522339094943\n",
    "\n",
    "Average log distance between runs: 0.012991759631576682\n",
    "Average log distance within runs: 0.02268019454745176\n",
    "Ratio of between within runs: 0.5728239942737341\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the model results against a taxonomy\n",
    "It necessitate access to a taxonomy for the dataset. Some of the subsequent steps depend on this taxonomy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.39 / 0.6975"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the taxonomy tree and ordering the clusters according to the tree\n",
    "htree_file = config[dataset][\"data_path\"] / config[dataset][\"htree_file\"]\n",
    "data[\"cluster\"] = np.array([c.strip() for c in data[\"cluster\"]])\n",
    "cats_ttypes = np.unique(data[\"cluster\"])\n",
    "C_ttypes = len(cats_ttypes)\n",
    "merged_cells_labels, treeobj, _ = get_merged_types(\n",
    "    htree_file=htree_file, cells_labels=data[\"cluster\"], num_classes=1\n",
    ")\n",
    "\n",
    "cluster_ids = []\n",
    "for i, s in enumerate(treeobj.child):\n",
    "    s = s.strip()\n",
    "    if (cats_ttypes == s).any():\n",
    "        cluster_id = list(cats_ttypes).index(s)\n",
    "        cluster_ids.append(cluster_id)\n",
    "\n",
    "cats_ttypes_sorted = cats_ttypes[cluster_ids]\n",
    "\n",
    "inds_ttypes_sorted = []\n",
    "for s in cats_ttypes_sorted:\n",
    "    inds_ttypes_sorted.append(list(cats_ttypes).index(s))\n",
    "inds_ttypes_sorted = np.array(inds_ttypes_sorted)\n",
    "\n",
    "# Plotting the taxonomy tree\n",
    "treeobj.plot(\n",
    "    figsize=[30, 10],\n",
    "    txtleafonly=True,\n",
    "    skeletononly=False,\n",
    "    fontsize=18,\n",
    "    skeletoncol=\"gray\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign colors to each cell type according to the taxonomy\n",
    "color_ord_T = treeobj.col[treeobj.isleaf]\n",
    "color_T = treeobj.col[treeobj.isleaf]\n",
    "data[\"cluster_color\"] = np.array([color_ord_T[0]] * len(data[\"cluster\"]))\n",
    "for i_type, ttype in enumerate(cats_ttypes):\n",
    "    idx1 = np.where(data[\"cluster\"] == ttype)[0]\n",
    "    idx2 = np.where(cats_ttypes_sorted == ttype)[0][0]\n",
    "    data[\"cluster_color\"][idx1] = color_ord_T[idx2]\n",
    "    color_T[i_type] = color_ord_T[idx2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome = summarize_inference(cplMixVAE, selected_model, all_loader)\n",
    "category_vs_class = np.zeros((n_arm, data[\"n_type\"], n_categories))\n",
    "\n",
    "for a in range(A):\n",
    "    label_predict = []\n",
    "    for d in range(len(data[\"cluster_id\"])):\n",
    "        z_cat = np.squeeze(outcome[\"c_prob\"][a][d, :])\n",
    "        category_vs_class[a, int(data[\"cluster_id\"][d] - 1), np.argmax(z_cat)] += 1\n",
    "cT_vs_cT = category_vs_class[:, :, outcome[\"nprune_indx\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### t-types vs. MMIDAS categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "PerformanceMatrix = np.ndarray\n",
    "ConsensusMatrix = np.ndarray\n",
    "\n",
    "\n",
    "def rev(x: np.ndarray) -> np.ndarray:\n",
    "    return x[::-1]\n",
    "\n",
    "\n",
    "@unstable\n",
    "def compute_consensus(pm: np.ndarray, target_dim: int) -> ConsensusMatrix:\n",
    "    C = pm.shape[-1]\n",
    "\n",
    "    cat_inds = linear_sum_assignment(-pm)[1]\n",
    "    sort_idx = np.vstack(\n",
    "        [rev(np.argsort(pm[c_t])) for c_t in range(target_dim)]\n",
    "    ).astype(int)\n",
    "    max_c_ind = -1 * np.ones(C)\n",
    "    if C > target_dim:\n",
    "        for c in filter(lambda c: c not in sort_idx[:, 0], range(C)):\n",
    "            col = np.argmax(pm[:, c].astype(int))\n",
    "            find_indx = np.where(sort_idx[col, 0] == sort_idx[:, 0])[0][0]\n",
    "            max_c_ind[find_indx + 1] = c\n",
    "        max_c_ind[max_c_ind == -1] = sort_idx[:, 0]\n",
    "        cat_inds = max_c_ind.astype(int)\n",
    "    return pm[:, cat_inds], cat_inds\n",
    "\n",
    "\n",
    "def normalize_consensus(pm: np.ndarray) -> ConsensusMatrix:\n",
    "    return pm / np.max(pm) / 2\n",
    "\n",
    "\n",
    "@unstable\n",
    "def plot_consensus(\n",
    "    cm: np.ndarray,\n",
    "    xticks,\n",
    "    yticks,\n",
    "    colors,\n",
    "    axes: tuple[tuple[Arm, Arms, Optional[Run]], tuple[Arm, Arms, Optional[Run]]],\n",
    "    savedir: Optional[str],\n",
    ") -> None:\n",
    "    C_targets = cm.shape[0]\n",
    "    C = cm.shape[1]\n",
    "\n",
    "    fig, axs = plt.subplots(1, 1, figsize=(12, 15), dpi=100)\n",
    "    for c_t in trange(C_targets):\n",
    "        for c in range(C):\n",
    "            axs.add_patch(\n",
    "                plt.Circle(np.array([c, c_t]), radius=(cm[c_t, c]), color=colors[c_t])\n",
    "            )\n",
    "    axs.set_xlim([-0.1, C + 0.1])\n",
    "    axs.set_ylim([-0.1, C_targets + 0.1])\n",
    "    axs.invert_yaxis()\n",
    "    axs.set_xlabel(f\"Categories for {get_axis(axes[0])}\", fontsize=20)\n",
    "    axs.set_xticks(np.arange(C))\n",
    "    axs.set_xticklabels(xticks, fontsize=6, rotation=90)\n",
    "    axs.set_yticks(np.arange(C_targets))\n",
    "    axs.set_yticklabels(yticks, fontsize=8)\n",
    "    fig.tight_layout()\n",
    "    if savedir:\n",
    "        plt.savefig(\n",
    "            savedir\n",
    "            + f\"/Taxonomy_{get_axis_save(axes[0])}_vs_{get_axis_save(axes[1])}.png\",\n",
    "            dpi=300,\n",
    "        )\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "for a in range(A):\n",
    "    pm, cat_inds = compute_consensus(\n",
    "        cT_vs_cT[a, inds_ttypes_sorted, :], len(cats_ttypes_sorted)\n",
    "    )\n",
    "    plot_consensus(\n",
    "        normalize_consensus(pm),\n",
    "        _ev[\"inds_unpruned\"][cat_inds],\n",
    "        cats_ttypes_sorted,\n",
    "        color_ord_T,\n",
    "        ((a, A, None), (\"ttypes\", \"ttypes\", None)),\n",
    "        saving_folder,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mutual Information between t-types and MMIDAS categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(probs: np.ndarray) -> np.ndarray:\n",
    "    return np.eye(probs.shape[-1])[np.argmax(probs, axis=-1)]\n",
    "\n",
    "\n",
    "def avg_mi(xs: np.ndarray) -> float:\n",
    "    return np.mean(np.max(xs, axis=-1)).item()\n",
    "\n",
    "\n",
    "@unstable\n",
    "def compute_mi(probs: np.ndarray, targets: np.ndarray) -> MI:\n",
    "    C_pred = probs.shape[-1]\n",
    "    C_target = targets.shape[-1]\n",
    "\n",
    "    preds = np.argmax(probs, axis=-1)\n",
    "    mi = np.zeros((C_target, C_pred))\n",
    "    for c_p in trange(C_pred):\n",
    "        for c_t in range(C_target):\n",
    "            mi[c_t, c_p] = adjusted_mutual_info_score(\n",
    "                targets[:, c_t], (preds == c_p).astype(int)\n",
    "            )\n",
    "    return mi\n",
    "\n",
    "\n",
    "def normalize_mi(mi: np.ndarray) -> np.ndarray:\n",
    "    return normalize(mi, axis=1, norm=\"l1\")\n",
    "\n",
    "\n",
    "def plot_mi(\n",
    "    mi: np.ndarray,\n",
    "    axes: tuple[tuple[Arm, Arms], tuple[Arm, Arms]],\n",
    "    xticks,\n",
    "    yticks,\n",
    "    savedir: Optional[str],\n",
    ") -> None:\n",
    "    _, col_inds = linear_sum_assignment(-mi)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 10), dpi=100)\n",
    "    plt.title(\n",
    "        f\"Mutual information for |c|= {mi.shape[1]} (avg={avg_mi(mi):.6f})\", fontsize=20\n",
    "    )\n",
    "    sns.set_theme(font_scale=1.0)\n",
    "    if xticks and yticks:\n",
    "        sns.heatmap(\n",
    "            mi[:, col_inds],\n",
    "            xticklabels=xticks[col_inds],\n",
    "            yticklabels=yticks,\n",
    "            vmin=0,\n",
    "            vmax=1,\n",
    "            ax=ax,\n",
    "            cbar_kws={\"shrink\": 1},\n",
    "        )  # pay attention to xticklabels\n",
    "    else:\n",
    "        sns.heatmap(mi[:, col_inds], vmin=0, vmax=1, ax=ax, cbar_kws={\"shrink\": 1})\n",
    "    ax.set_xlabel(get_axis(axes[0]), fontsize=20)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), fontsize=8)\n",
    "    ax.set_ylabel(get_axis(axes[1]), fontsize=20)\n",
    "    ax.set_yticks(np.arange(len(yticks)))\n",
    "    ax.set_yticklabels(yticks, fontsize=8)\n",
    "    fig.tight_layout()\n",
    "    if savedir:\n",
    "        plt.savefig(\n",
    "            savedir\n",
    "            + f\"/mutinfo_{get_axis_save(axes[0])}_vs_{get_axis_save(axes[1])}_K{mi.shape[1]}.png\",\n",
    "            dpi=300,\n",
    "        )  # TODO\n",
    "    plt.show()\n",
    "\n",
    "@unstable\n",
    "def compute_mis(probs: tuple[np.ndarray, np.ndarray]):\n",
    "    cs_a, cs_b = probs\n",
    "    A = len(cs_a)\n",
    "    B = len(cs_b)\n",
    "\n",
    "    avgs = np.zeros((A, B))\n",
    "    for a in range(A):\n",
    "        for b in range(B):\n",
    "            avgs[a, b] = avg_mi(compute_mi(cs_a[a], one_hot(cs_b[b])))\n",
    "    return avgs\n",
    "\n",
    "@unstable\n",
    "# TODO\n",
    "def plot_mi_matrix(avgs: np.ndarray):\n",
    "    A, B = avgs.shape\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 10), dpi=100)\n",
    "    plt.title(f\"Mutual information matrix for {A} vs {B} arms\", fontsize=20)\n",
    "    sns.set_theme(font_scale=1.0)\n",
    "    sns.heatmap(avgs, vmin=0, vmax=1, ax=ax, cbar_kws={\"shrink\": 1})\n",
    "    ax.set_xlabel(\"Arm A\", fontsize=20)\n",
    "    ax.set_xticks(np.arange(A))\n",
    "    ax.set_xticklabels(np.arange(A), fontsize=8)\n",
    "    ax.set_ylabel(\"Arm B\", fontsize=20)\n",
    "    ax.set_yticks(np.arange(B))\n",
    "    ax.set_yticklabels(np.arange(B), fontsize=8)\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "so_far = 0\n",
    "so_far_a = 0\n",
    "so_far_b = 0\n",
    "for a in range(A):\n",
    "    for b in range(B):\n",
    "        mi = compute_mi(ev[\"cs_a\"][a], one_hot(ev[\"cs_b\"][b]))\n",
    "        plot_mi(\n",
    "            normalize_mi(mi),\n",
    "            ((a, A, ra), (b, B, rb)),\n",
    "            [],\n",
    "            [],\n",
    "            f\"multiarm-results/{A}{B}\"\n",
    "            # None\n",
    "        )\n",
    "        so_far += 1\n",
    "\n",
    "    for b in range(a + 1, A):\n",
    "        mi = compute_mi(ev[\"cs_a\"][a], one_hot(ev[\"cs_a\"][b]))\n",
    "        plot_mi(\n",
    "            normalize_mi(mi),\n",
    "            ((a, A, ra), (b, A, ra)),\n",
    "            [],\n",
    "            [],\n",
    "            f\"multiarm-results/{A}{B}\"\n",
    "            # None\n",
    "        )\n",
    "        so_far_a += 1\n",
    "\n",
    "for a in range(B):\n",
    "    for b in range(a + 1, B):\n",
    "        mi = compute_mi(ev[\"cs_b\"][a], one_hot(ev[\"cs_b\"][b]))\n",
    "        plot_mi(\n",
    "            mi,\n",
    "            ((a, B, rb), (b, B, rb)),\n",
    "            [],\n",
    "            [],\n",
    "            f\"multiarm-results/{A}{B}\"\n",
    "            # None\n",
    "        )\n",
    "        so_far_b += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_state():\n",
    "    raise NotImplementedError\n",
    "    _, data_index = all_loader.dataset.tensors\n",
    "    data_index = data_index.numpy().astype(int)\n",
    "\n",
    "    for arm in range(n_arm):\n",
    "        print(arm)\n",
    "        plt.close(\"all\")\n",
    "        fig = plt.figure(figsize=(5, 5))\n",
    "        m_size = 3\n",
    "        alp = 0.5\n",
    "        fontsize = 18\n",
    "\n",
    "        sns.set_theme()\n",
    "        sns.set(rc={\"axes.facecolor\": \"whitesmoke\"})\n",
    "\n",
    "        if state_dim == 1:\n",
    "            axs = fig.add_subplot(1, 1, 1)\n",
    "            axs.hist(\n",
    "                outcome[\"state_mu\"][arm][:, 0],\n",
    "                color=data[\"cluster_color\"][data_index],\n",
    "                s=m_size,\n",
    "                alpha=alp,\n",
    "            )\n",
    "            axs.set_xlabel(f\"T_s_{0}\")\n",
    "        elif state_dim == 2:\n",
    "            axs = fig.add_subplot(1, 1, 1)\n",
    "            axs.scatter(\n",
    "                outcome[\"state_mu\"][arm][:, 0],\n",
    "                outcome[\"state_mu\"][arm][:, 1],\n",
    "                color=data[\"cluster_color\"][data_index],\n",
    "                s=m_size,\n",
    "                alpha=alp,\n",
    "            )\n",
    "            axs.set_xlabel(r\"$s_{T_1}$\", fontsize=fontsize)\n",
    "            axs.set_ylabel(r\"$s_{T_2}$\", fontsize=fontsize)\n",
    "            axs.xaxis.set_tick_params(labelsize=10)\n",
    "            axs.yaxis.set_tick_params(labelsize=10)\n",
    "        elif state_dim == 3:\n",
    "            axs = fig.add_subplot(1, 1, 1, projection=\"3d\")\n",
    "            axs.scatter(\n",
    "                outcome[\"state_mu\"][arm][:, 0],\n",
    "                outcome[\"state_mu\"][arm][:, 1],\n",
    "                outcome[\"state_mu\"][arm][:, 2],\n",
    "                color=data[\"cluster_color\"][data_index],\n",
    "                s=m_size,\n",
    "                alpha=alp,\n",
    "            )\n",
    "            axs.set_xlabel(f\"T_s_{0}\")\n",
    "            axs.set_ylabel(f\"T_s_{1}\")\n",
    "            axs.set_zlabel(f\"T_s_{2}\")\n",
    "\n",
    "        axs.set_ylim([-2.5, 3])\n",
    "        axs.set_xlim([-2.5, 2.5])\n",
    "        axs.set_title(f\"Continuous Representation (arm {arm+1})\", fontsize=18, pad=15)\n",
    "        axs.grid(False)\n",
    "        fig.tight_layout()\n",
    "        plt.savefig(saving_folder + f\"/state_mu_K_{model_order}_arm_{arm}.png\", dpi=600)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def empirical_distribution(x):\n",
    "    x = np.argmax(x, axis=-1)\n",
    "    \n",
    "    # [[0.2, 0.7, 0.1], [0.5, 0.4, 0.1]] |-> [[0.0, 1.0, 0.0], [1.0, 0.0, 0.0]] |-> [[0.5, 0.5, 0.0]]\n",
    "\n",
    "\n",
    "def kl_dist(P: np.ndarray, Q: np.ndarray, eps=1e-9) -> np.float64:\n",
    "    P = np.clip(P, eps, 1)\n",
    "    Q = np.clip(Q, eps, 1)\n",
    "    return np.sum(P * np.log(P / Q), axis=-1)\n",
    "\n",
    "def test_kl_simple():\n",
    "    P = np.array([0.5, 0.5])\n",
    "    Q = np.array([0.9, 0.1])\n",
    "\n",
    "    kl = kl_dist(P, Q)\n",
    "    print(f\"kl(P, Q): {kl:.4f}\")\n",
    "\n",
    "    # Analytical computation\n",
    "    kl_analytical = 0.5 * np.log(0.5 / 0.9) + 0.5 * np.log(0.5 / 0.1)\n",
    "    print(f\"analytical kl: {kl_analytical:.4f}\")\n",
    "\n",
    "    assert np.isclose(kl, kl_analytical), \"Test failed: The computed KL divergence does not match the analytical value.\"\n",
    "    print(\"Test passed: Simple distributions.\")\n",
    "\n",
    "def test_kl_zero():\n",
    "    P = [0.4, 0.6, 0.0]\n",
    "    Q = [0.5, 0.5, 0.0]\n",
    "\n",
    "    kl = kl_dist(P, Q)\n",
    "    print(f\"KL Divergence with zero probabilities: {kl:.4f}\")\n",
    "\n",
    "    # Since P[2] and Q[2] are zero, they should not contribute to the KL divergence\n",
    "    kl_expected = 0.4 * np.log(0.4 / 0.5) + 0.6 * np.log(0.6 / 0.5)\n",
    "    print(f\"Expected KL Divergence: {kl_expected:.4f}\")\n",
    "\n",
    "    assert np.isclose(kl, kl_expected), \"Test failed: The computed KL divergence does not match the expected value with zero probabilities.\"\n",
    "    print(\"Test passed: Zero probabilities.\")\n",
    "\n",
    "def test_kl_identical():\n",
    "    P = np.array([0.2, 0.3, 0.5])\n",
    "    Q = np.array([0.2, 0.3, 0.5])\n",
    "\n",
    "    kl = kl_dist(P, Q)\n",
    "    print(f\"KL Divergence between identical distributions: {kl:.4f}\")\n",
    "\n",
    "    assert np.isclose(kl, 0), \"Test failed: KL divergence between identical distributions should be zero.\"\n",
    "    print(\"Test passed: Identical distributions.\")\n",
    "\n",
    "def test_kl_random():\n",
    "    np.random.seed(42)\n",
    "    P = np.random.rand(10)\n",
    "    Q = np.random.rand(10)\n",
    "\n",
    "    kl = kl_dist(P, Q)\n",
    "    print(f\"KL Divergence between random distributions: {kl:.4f}\")\n",
    "\n",
    "    assert kl >= 0, \"Test failed: KL divergence should be non-negative.\"\n",
    "    print(\"Test passed: Random distributions.\")\n",
    "\n",
    "test_kl_simple()\n",
    "test_kl_zero()\n",
    "test_kl_identical()\n",
    "test_kl_random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_kl(P, Q):\n",
    "    return 0.5 * (kl_dist(P, Q) + kl_dist(Q, P))\n",
    "\n",
    "def test_avgkl_simple():\n",
    "    P = np.array([0.5, 0.5])\n",
    "    Q = np.array([0.9, 0.1])\n",
    "\n",
    "    avg_kl_div = avg_kl(P, Q)\n",
    "    print(f\"Average KL Divergence between P and Q: {avg_kl_div:.4f}\")\n",
    "\n",
    "    # Analytical computation\n",
    "    kl_PQ = 0.5 * np.log(0.5 / 0.9) + 0.5 * np.log(0.5 / 0.1)\n",
    "    kl_QP = 0.9 * np.log(0.9 / 0.5) + 0.1 * np.log(0.1 / 0.5)\n",
    "    avg_kl_analytical = 0.5 * (kl_PQ + kl_QP)\n",
    "    print(f\"Analytical Average KL Divergence: {avg_kl_analytical:.4f}\")\n",
    "\n",
    "    assert np.isclose(avg_kl_div, avg_kl_analytical), \"Test failed: The computed average KL divergence does not match the analytical value.\"\n",
    "    print(\"Test passed: Simple distributions.\")\n",
    "\n",
    "def test_avgkl_identical():\n",
    "    P = np.array([0.2, 0.3, 0.5])\n",
    "    Q = np.array([0.2, 0.3, 0.5])\n",
    "\n",
    "    avg_kl_div = avg_kl(P, Q)\n",
    "    print(f\"Average KL Divergence between identical distributions: {avg_kl_div:.4f}\")\n",
    "\n",
    "    assert np.isclose(avg_kl_div, 0), \"Test failed: Average KL divergence between identical distributions should be zero.\"\n",
    "    print(\"Test passed: Identical distributions.\")\n",
    "\n",
    "def test_avgkl_zero():\n",
    "    P = [0.4, 0.6, 0.0]\n",
    "    Q = [0.5, 0.5, 0.0]\n",
    "\n",
    "    avg_kl_div = avg_kl(P, Q)\n",
    "    print(f\"Average KL Divergence with zero probabilities: {avg_kl_div:.4f}\")\n",
    "\n",
    "    # Since P[2] and Q[2] are zero, they should not contribute to the KL divergence\n",
    "    kl_PQ = 0.4 * np.log(0.4 / 0.5) + 0.6 * np.log(0.6 / 0.5)\n",
    "    kl_QP = 0.5 * np.log(0.5 / 0.4) + 0.5 * np.log(0.5 / 0.6)\n",
    "    avg_kl_expected = 0.5 * (kl_PQ + kl_QP)\n",
    "    print(f\"Expected Average KL Divergence: {avg_kl_expected:.4f}\")\n",
    "\n",
    "    assert np.isclose(avg_kl_div, avg_kl_expected), \"Test failed: The computed average KL divergence does not match the expected value with zero probabilities.\"\n",
    "    print(\"Test passed: Zero probabilities.\")\n",
    "\n",
    "def test_avgkl_random():\n",
    "    np.random.seed(42)\n",
    "    P = np.random.rand(10)\n",
    "    Q = np.random.rand(10)\n",
    "\n",
    "    avg_kl_div = avg_kl(P, Q)\n",
    "    print(f\"Average KL Divergence between random distributions: {avg_kl_div:.4f}\")\n",
    "\n",
    "    assert avg_kl_div >= 0, \"Test failed: Average KL divergence should be non-negative.\"\n",
    "    print(\"Test passed: Random distributions.\")\n",
    "\n",
    "def test_avgkl_symmetry():\n",
    "    P = [0.3, 0.7]\n",
    "    Q = [0.6, 0.4]\n",
    "\n",
    "    avg_kl_PQ = avg_kl(P, Q)\n",
    "    avg_kl_QP = avg_kl(Q, P)\n",
    "    print(f\"Average KL Divergence avg_kl(P, Q): {avg_kl_PQ:.4f}\")\n",
    "    print(f\"Average KL Divergence avg_kl(Q, P): {avg_kl_QP:.4f}\")\n",
    "\n",
    "    assert np.isclose(avg_kl_PQ, avg_kl_QP), \"Test failed: Average KL divergence should be symmetric.\"\n",
    "    print(\"Test passed: Symmetry check.\")\n",
    "\n",
    "test_avgkl_simple()\n",
    "test_avgkl_identical()\n",
    "test_avgkl_zero()\n",
    "test_avgkl_random()\n",
    "test_avgkl_symmetry()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_kl(\n",
    "    kl: np.ndarray,\n",
    "    axes: tuple[tuple[Arm, Arms, Run], tuple[Arm, Arms, Run]],\n",
    "    savedir: Optional[str],\n",
    ") -> None:\n",
    "        \n",
    "    _a, _b = axes\n",
    "    arma, armsa, ra = _a\n",
    "    armb, armsb, rb = _b\n",
    "\n",
    "    # Create heatmap\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "    ax.set_title(\n",
    "        f\"Avg. KL Divergence for |c|= {92}, Arm {arma} of {armsa}-arm MMIDAS run {ra}, Arm {armb} of {armsb}-arm MMIDAS run {rb} (avg={np.mean(kl):.4f})\", fontsize=20\n",
    "    )\n",
    "    ax.set_xlabel('KL Divergence')\n",
    "    ax.set_ylabel('Data Points')\n",
    "    im3 = ax.imshow(kl.reshape(-1, 1), aspect='auto', cmap='hot')\n",
    "    plt.colorbar(im3, ax=ax)\n",
    "    plt.tight_layout()\n",
    "    if savedir:\n",
    "        plt.savefig(\n",
    "            savedir\n",
    "            + f\"/kl_{get_axis_save(axes[0])}_vs_{get_axis_save(axes[1])}_K{92}.png\",\n",
    "            dpi=300,\n",
    "            bbox_inches='tight'\n",
    "        )\n",
    "    plt.show()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "so_far = 0\n",
    "so_far_a = 0\n",
    "so_far_b = 0\n",
    "for a in range(A):\n",
    "    for b in range(B):\n",
    "        kl = avg_kl(ev[\"cs_a\"][a], ev[\"cs_b\"][b])\n",
    "        plot_kl(\n",
    "            kl,\n",
    "            ((a, A, ra), (b, B, rb)),\n",
    "            f\"multiarm-results/{A}{B}\"\n",
    "            # None\n",
    "        )\n",
    "        so_far += 1\n",
    "\n",
    "    for b in range(a + 1, A):\n",
    "        kl = avg_kl(ev[\"cs_a\"][a], ev[\"cs_a\"][b])\n",
    "        plot_kl(\n",
    "            kl,\n",
    "            ((a, A, ra), (b, A, ra)),\n",
    "            f\"multiarm-results/{A}{B}\"\n",
    "            # None\n",
    "        )\n",
    "        so_far_a += 1\n",
    "\n",
    "for a in range(B):\n",
    "    for b in range(a + 1, B):\n",
    "        kl = avg_kl(ev[\"cs_b\"][a], ev[\"cs_b\"][b])\n",
    "        plot_kl(\n",
    "            kl,\n",
    "            ((a, B, rb), (b, B, rb)),\n",
    "            f\"multiarm-results/{A}{B}\"\n",
    "            # None\n",
    "        )\n",
    "        so_far_b += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
