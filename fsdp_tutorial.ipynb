{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial is going to show you how to use PyTorch's FSDP implementation. We're going to train two models: (i) a shallow model, and (ii) a deep model with local training. Then, we're going to show how FSDP speeds up our training, and decreases memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms # for datasets\n",
    "from tqdm import tqdm # for progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# params:\n",
      "  shallow: 1199882\n",
      "  deep: 94104234\n"
     ]
    }
   ],
   "source": [
    "class ShallowNet(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(ShallowNet, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "    self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "    self.dropout1 = nn.Dropout(0.25)\n",
    "    self.dropout2 = nn.Dropout(0.5)\n",
    "    self.fc1 = nn.Linear(9216, 128)\n",
    "    self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.conv1(x)\n",
    "    x = F.relu(x)\n",
    "    x = self.conv2(x)\n",
    "    x = F.relu(x)\n",
    "    x = F.max_pool2d(x, 2)\n",
    "    x = self.dropout1(x)\n",
    "    x = torch.flatten(x, 1)\n",
    "    x = self.fc1(x)\n",
    "    x = F.relu(x)\n",
    "    x = self.dropout2(x)\n",
    "    x = self.fc2(x)\n",
    "    output = F.log_softmax(x, dim=1)\n",
    "    return output\n",
    "  \n",
    "class DeepNet(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(DeepNet, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "    self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "    self.dropout1 = nn.Dropout(0.25)\n",
    "    self.dropout2 = nn.Dropout(0.5)\n",
    "    self.fc1 = nn.Linear(9216, 9000)\n",
    "    self.fc1a = nn.Linear(9000, 1000)\n",
    "    self.fc1b = nn.Linear(1000, 1000)\n",
    "    self.fc1c = nn.Linear(1000, 1000)\n",
    "    self.fc1d = nn.Linear(1000, 128)\n",
    "    self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.conv1(x)\n",
    "    x = F.relu(x)\n",
    "    x = self.conv2(x)\n",
    "    x = F.relu(x)\n",
    "    x = F.max_pool2d(x, 2)\n",
    "    x = self.dropout1(x)\n",
    "    x = torch.flatten(x, 1)\n",
    "    x = self.fc1(x)\n",
    "    x = F.relu(x)\n",
    "    x = self.dropout2(x)\n",
    "    x = self.fc1a(x)\n",
    "    x = self.fc1b(x)\n",
    "    x = self.fc1c(x)\n",
    "    x = self.fc1d(x)\n",
    "    x = self.fc2(x)\n",
    "    output = F.log_softmax(x, dim=1)\n",
    "    return output\n",
    "  \n",
    "print(\"# params:\")\n",
    "print(f\"  shallow: {count_params(ShallowNet())}\")\n",
    "print(f\"  deep: {count_params(DeepNet())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset MNIST\n",
       "     Number of datapoints: 60000\n",
       "     Root location: ../data\n",
       "     Split: Train\n",
       "     StandardTransform\n",
       " Transform: Compose(\n",
       "                ToTensor()\n",
       "                Normalize(mean=(0.1307,), std=(0.3081,))\n",
       "            ),\n",
       " Dataset MNIST\n",
       "     Number of datapoints: 10000\n",
       "     Root location: ../data\n",
       "     Split: Test\n",
       "     StandardTransform\n",
       " Transform: Compose(\n",
       "                ToTensor()\n",
       "                Normalize(mean=(0.1307,), std=(0.3081,))\n",
       "            ))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_transform = transforms.Compose([\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize((0.1307,), (0.3081,))\n",
    "  ])\n",
    "\n",
    "train_data = datasets.MNIST('../data', train=True, download=True, transform=_transform)\n",
    "test_data = datasets.MNIST('../data', train=False, transform=_transform)\n",
    "\n",
    "train_data, test_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([tensor([[[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            ...,\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            ...,\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            ...,\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
       "  \n",
       "  \n",
       "          ...,\n",
       "  \n",
       "  \n",
       "          [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            ...,\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            ...,\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            ...,\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]]]),\n",
       "  tensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0, 9, 1,\n",
       "          1, 2, 4, 3, 2, 7, 3, 8, 6, 9, 0, 5, 6, 0, 7, 6, 1, 8, 7, 9, 3, 9, 8, 5,\n",
       "          9, 3, 3, 0, 7, 4, 9, 8, 0, 9, 4, 1, 4, 4, 6, 0, 4, 5, 6, 1, 0, 0, 1, 7,\n",
       "          1, 6, 3, 0, 2, 1, 1, 7, 9, 0, 2, 6, 7, 8, 3, 9, 0, 4, 6, 7, 4, 6, 8, 0,\n",
       "          7, 8, 3, 1, 5, 7, 1, 7, 1, 1, 6, 3, 0, 2, 9, 3, 1, 1, 0, 4, 9, 2, 0, 0,\n",
       "          2, 0, 2, 7, 1, 8, 6, 4, 1, 6, 3, 4, 5, 9, 1, 3, 3, 8, 5, 4, 7, 7, 4, 2,\n",
       "          8, 5, 8, 6, 7, 3, 4, 6, 1, 9, 9, 6, 0, 3, 7, 2, 8, 2, 9, 4, 4, 6, 4, 9,\n",
       "          7, 0, 9, 2, 9, 5, 1, 5, 9, 1, 2, 3, 2, 3, 5, 9, 1, 7, 6, 2, 8, 2, 2, 5,\n",
       "          0, 7, 4, 9, 7, 8, 3, 2, 1, 1, 8, 3, 6, 1, 0, 3, 1, 0, 0, 1, 7, 2, 7, 3,\n",
       "          0, 4, 6, 5, 2, 6, 4, 7, 1, 8, 9, 9, 3, 0, 7, 1, 0, 2, 0, 3, 5, 4, 6, 5,\n",
       "          8, 6, 3, 7, 5, 8, 0, 9, 1, 0, 3, 1, 2, 2, 3, 3])],\n",
       " [tensor([[[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            ...,\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            ...,\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            ...,\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
       "  \n",
       "  \n",
       "          ...,\n",
       "  \n",
       "  \n",
       "          [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            ...,\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            ...,\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            ...,\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]]]),\n",
       "  tensor([7, 2, 1, 0, 4, 1, 4, 9, 5, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4, 9, 6, 6, 5,\n",
       "          4, 0, 7, 4, 0, 1, 3, 1, 3, 4, 7, 2, 7, 1, 2, 1, 1, 7, 4, 2, 3, 5, 1, 2,\n",
       "          4, 4, 6, 3, 5, 5, 6, 0, 4, 1, 9, 5, 7, 8, 9, 3, 7, 4, 6, 4, 3, 0, 7, 0,\n",
       "          2, 9, 1, 7, 3, 2, 9, 7, 7, 6, 2, 7, 8, 4, 7, 3, 6, 1, 3, 6, 9, 3, 1, 4,\n",
       "          1, 7, 6, 9, 6, 0, 5, 4, 9, 9, 2, 1, 9, 4, 8, 7, 3, 9, 7, 4, 4, 4, 9, 2,\n",
       "          5, 4, 7, 6, 7, 9, 0, 5, 8, 5, 6, 6, 5, 7, 8, 1, 0, 1, 6, 4, 6, 7, 3, 1,\n",
       "          7, 1, 8, 2, 0, 2, 9, 9, 5, 5, 1, 5, 6, 0, 3, 4, 4, 6, 5, 4, 6, 5, 4, 5,\n",
       "          1, 4, 4, 7, 2, 3, 2, 7, 1, 8, 1, 8, 1, 8, 5, 0, 8, 9, 2, 5, 0, 1, 1, 1,\n",
       "          0, 9, 0, 3, 1, 6, 4, 2, 3, 6, 1, 1, 1, 3, 9, 5, 2, 9, 4, 5, 9, 3, 9, 0,\n",
       "          3, 6, 5, 5, 7, 2, 2, 7, 1, 2, 8, 4, 1, 7, 3, 3, 8, 8, 7, 9, 2, 2, 4, 1,\n",
       "          5, 9, 8, 7, 2, 3, 0, 4, 4, 2, 4, 1, 9, 5, 7, 7, 2, 8, 2, 6, 8, 5, 7, 7,\n",
       "          9, 1, 8, 1, 8, 0, 3, 0, 1, 9, 9, 4, 1, 8, 2, 1, 2, 9, 7, 5, 9, 2, 6, 4,\n",
       "          1, 5, 8, 2, 9, 2, 0, 4, 0, 0, 2, 8, 4, 7, 1, 2, 4, 0, 2, 7, 4, 3, 3, 0,\n",
       "          0, 3, 1, 9, 6, 5, 2, 5, 9, 2, 9, 3, 0, 4, 2, 0, 7, 1, 1, 2, 1, 5, 3, 3,\n",
       "          9, 7, 8, 6, 5, 6, 1, 3, 8, 1, 0, 5, 1, 3, 1, 5, 5, 6, 1, 8, 5, 1, 7, 9,\n",
       "          4, 6, 2, 2, 5, 0, 6, 5, 6, 3, 7, 2, 0, 8, 8, 5, 4, 1, 1, 4, 0, 3, 3, 7,\n",
       "          6, 1, 6, 2, 1, 9, 2, 8, 6, 1, 9, 5, 2, 5, 4, 4, 2, 8, 3, 8, 2, 4, 5, 0,\n",
       "          3, 1, 7, 7, 5, 7, 9, 7, 1, 9, 2, 1, 4, 2, 9, 2, 0, 4, 9, 1, 4, 8, 1, 8,\n",
       "          4, 5, 9, 8, 8, 3, 7, 6, 0, 0, 3, 0, 2, 6, 6, 4, 9, 3, 3, 3, 2, 3, 9, 1,\n",
       "          2, 6, 8, 0, 5, 6, 6, 6, 3, 8, 8, 2, 7, 5, 8, 9, 6, 1, 8, 4, 1, 2, 5, 9,\n",
       "          1, 9, 7, 5, 4, 0, 8, 9, 9, 1, 0, 5, 2, 3, 7, 8, 9, 4, 0, 6, 3, 9, 5, 2,\n",
       "          1, 3, 1, 3, 6, 5, 7, 4, 2, 2, 6, 3, 2, 6, 5, 4, 8, 9, 7, 1, 3, 0, 3, 8,\n",
       "          3, 1, 9, 3, 4, 4, 6, 4, 2, 1, 8, 2, 5, 4, 8, 8, 4, 0, 0, 2, 3, 2, 7, 7,\n",
       "          0, 8, 7, 4, 4, 7, 9, 6, 9, 0, 9, 8, 0, 4, 6, 0, 6, 3, 5, 4, 8, 3, 3, 9,\n",
       "          3, 3, 3, 7, 8, 0, 8, 2, 1, 7, 0, 6, 5, 4, 3, 8, 0, 9, 6, 3, 8, 0, 9, 9,\n",
       "          6, 8, 6, 8, 5, 7, 8, 6, 0, 2, 4, 0, 2, 2, 3, 1, 9, 7, 5, 1, 0, 8, 4, 6,\n",
       "          2, 6, 7, 9, 3, 2, 9, 8, 2, 2, 9, 2, 7, 3, 5, 9, 1, 8, 0, 2, 0, 5, 2, 1,\n",
       "          3, 7, 6, 7, 1, 2, 5, 8, 0, 3, 7, 2, 4, 0, 9, 1, 8, 6, 7, 7, 4, 3, 4, 9,\n",
       "          1, 9, 5, 1, 7, 3, 9, 7, 6, 9, 1, 3, 7, 8, 3, 3, 6, 7, 2, 8, 5, 8, 5, 1,\n",
       "          1, 4, 4, 3, 1, 0, 7, 7, 0, 7, 9, 4, 4, 8, 5, 5, 4, 0, 8, 2, 1, 0, 8, 4,\n",
       "          5, 0, 4, 0, 6, 1, 7, 3, 2, 6, 7, 2, 6, 9, 3, 1, 4, 6, 2, 5, 4, 2, 0, 6,\n",
       "          2, 1, 7, 3, 4, 1, 0, 5, 4, 3, 1, 1, 7, 4, 9, 9, 4, 8, 4, 0, 2, 4, 5, 1,\n",
       "          1, 6, 4, 7, 1, 9, 4, 2, 4, 1, 5, 5, 3, 8, 3, 1, 4, 5, 6, 8, 9, 4, 1, 5,\n",
       "          3, 8, 0, 3, 2, 5, 1, 2, 8, 3, 4, 4, 0, 8, 8, 3, 3, 1, 7, 3, 5, 9, 6, 3,\n",
       "          2, 6, 1, 3, 6, 0, 7, 2, 1, 7, 1, 4, 2, 4, 2, 1, 7, 9, 6, 1, 1, 2, 4, 8,\n",
       "          1, 7, 7, 4, 8, 0, 7, 3, 1, 3, 1, 0, 7, 7, 0, 3, 5, 5, 2, 7, 6, 6, 9, 2,\n",
       "          8, 3, 5, 2, 2, 5, 6, 0, 8, 2, 9, 2, 8, 8, 8, 8, 7, 4, 9, 3, 0, 6, 6, 3,\n",
       "          2, 1, 3, 2, 2, 9, 3, 0, 0, 5, 7, 8, 1, 4, 4, 6, 0, 2, 9, 1, 4, 7, 4, 7,\n",
       "          3, 9, 8, 8, 4, 7, 1, 2, 1, 2, 2, 3, 2, 3, 2, 3, 9, 1, 7, 4, 0, 3, 5, 5,\n",
       "          8, 6, 3, 2, 6, 7, 6, 6, 3, 2, 7, 8, 1, 1, 7, 5, 6, 4, 9, 5, 1, 3, 3, 4,\n",
       "          7, 8, 9, 1, 1, 6, 9, 1, 4, 4, 5, 4, 0, 6, 2, 2, 3, 1, 5, 1, 2, 0, 3, 8,\n",
       "          1, 2, 6, 7, 1, 6, 2, 3, 9, 0, 1, 2, 2, 0, 8, 9])])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch_size = 256\n",
    "test_batch_size = 1000\n",
    "\n",
    "\n",
    "loader_kwargs = {\n",
    "    'num_workers': 2,\n",
    "    'pin_memory': True,\n",
    "    'shuffle': False,\n",
    "    'drop_last': True,\n",
    "    'persistent_workers': True, # warning: on Allen HPC, disabling this massively slows down training\n",
    "}\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=train_batch_size, **loader_kwargs)\n",
    "test_loader = DataLoader(test_data, batch_size=test_batch_size, **loader_kwargs)\n",
    "\n",
    "next(iter(train_loader)), next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, opt, device):\n",
    "  total_loss = 0\n",
    "  num_batches = 0\n",
    "\n",
    "  model.train()\n",
    "  for (data, target) in tqdm(train_loader, total=len(train_loader)):\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    opt.zero_grad()\n",
    "    output = model(data)\n",
    "    loss = F.nll_loss(output, target, reduction='sum')\n",
    "    memory = torch.cuda.memory_allocated(device) / 1e6 if torch.cuda.is_available() else 'N/A' # memory in MB\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    total_loss += loss.item()\n",
    "    num_batches += 1\n",
    "\n",
    "  avg_loss = total_loss / (num_batches * train_loader.batch_size) # average loss per datapoint\n",
    "  return {\n",
    "    'avg_loss': avg_loss,\n",
    "    'memory': memory\n",
    "  }\n",
    "\n",
    "def test(model, test_loader, device):\n",
    "  total_loss = 0\n",
    "  num_batches = 0\n",
    "  total_correct = 0\n",
    "  num_datapoints = 0\n",
    "\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    for (data, target) in tqdm(test_loader, total=len(test_loader)):\n",
    "      data, target = data.to(device), target.to(device)\n",
    "      output = model(data)\n",
    "      total_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "      pred = output.argmax(dim=1, keepdim=True)\n",
    "      total_correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "      num_datapoints += len(data)\n",
    "      num_batches += 1\n",
    "  \n",
    "  avg_loss = total_loss / (num_batches * test_loader.batch_size) # average loss per datapoint\n",
    "  accuracy = total_correct / num_datapoints\n",
    "  return {\n",
    "    'avg_loss': avg_loss, \n",
    "    'total_correct': total_correct, \n",
    "    'num_datapoints': num_datapoints, \n",
    "    'accuracy': accuracy\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> training shallow model\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 234/234 [00:02<00:00, 86.32it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 32.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 | train loss: 0.2456 | test loss: 0.0529 | test acc: 9833/10000 (0.9833) | memory: N/A MB\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 234/234 [00:02<00:00, 88.83it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 57.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 | train loss: 0.0810 | test loss: 0.0384 | test acc: 9866/10000 (0.9866) | memory: N/A MB\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 234/234 [00:02<00:00, 89.39it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 56.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 | train loss: 0.0628 | test loss: 0.0316 | test acc: 9896/10000 (0.9896) | memory: N/A MB\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 234/234 [00:02<00:00, 89.03it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 55.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 | train loss: 0.0501 | test loss: 0.0293 | test acc: 9895/10000 (0.9895) | memory: N/A MB\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 234/234 [00:02<00:00, 85.63it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 54.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 | train loss: 0.0439 | test loss: 0.0309 | test acc: 9898/10000 (0.9898) | memory: N/A MB\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 234/234 [00:02<00:00, 88.13it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 55.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 | train loss: 0.0369 | test loss: 0.0314 | test acc: 9900/10000 (0.9900) | memory: N/A MB\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 234/234 [00:02<00:00, 86.92it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 56.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 | train loss: 0.0310 | test loss: 0.0297 | test acc: 9896/10000 (0.9896) | memory: N/A MB\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 234/234 [00:02<00:00, 87.90it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 53.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 | train loss: 0.0298 | test loss: 0.0401 | test acc: 9878/10000 (0.9878) | memory: N/A MB\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 234/234 [00:02<00:00, 83.84it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 53.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 | train loss: 0.0270 | test loss: 0.0320 | test acc: 9907/10000 (0.9907) | memory: N/A MB\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 234/234 [00:02<00:00, 82.25it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 37.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 | train loss: 0.0260 | test loss: 0.0325 | test acc: 9895/10000 (0.9895) | memory: N/A MB\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "lr = 0.001\n",
    "\n",
    "shallow_model = ShallowNet().to(device)\n",
    "shallow_optimizer = optim.Adam(shallow_model.parameters(), lr=lr)\n",
    "\n",
    "print(\"> training shallow model\\n\")\n",
    "for epoch in range(epochs):\n",
    "  train_losses = train(shallow_model, train_loader, shallow_optimizer, device)\n",
    "  test_losses = test(shallow_model, test_loader, device)\n",
    "  print(f\"epoch: {epoch} | train loss: {train_losses['avg_loss']:.4f} | test loss: {test_losses['avg_loss']:.4f} | test acc: {test_losses['total_correct']}/{test_losses['num_datapoints']} ({test_losses['accuracy']:.4f}) | memory: {train_losses['memory']} MB\")\n",
    "  print('-' * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> training deep model\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 234/234 [00:23<00:00, 10.12it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 13.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 | train loss: 0.3304 | test loss: 0.0587 | test acc: 9798/10000 (0.9798) | memory: N/A MB\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 234/234 [00:23<00:00, 10.17it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 13.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 | train loss: 0.0649 | test loss: 0.0628 | test acc: 9814/10000 (0.9814) | memory: N/A MB\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 234/234 [00:23<00:00, 10.04it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 14.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 | train loss: 0.0406 | test loss: 0.0481 | test acc: 9864/10000 (0.9864) | memory: N/A MB\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 234/234 [00:23<00:00, 10.10it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 14.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 | train loss: 0.0325 | test loss: 0.0480 | test acc: 9869/10000 (0.9869) | memory: N/A MB\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 234/234 [00:23<00:00,  9.90it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 12.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 | train loss: 0.0303 | test loss: 0.0563 | test acc: 9843/10000 (0.9843) | memory: N/A MB\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 234/234 [00:23<00:00, 10.09it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 14.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 | train loss: 0.0233 | test loss: 0.0478 | test acc: 9895/10000 (0.9895) | memory: N/A MB\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 234/234 [00:23<00:00, 10.17it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 14.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 | train loss: 0.0253 | test loss: 0.0436 | test acc: 9894/10000 (0.9894) | memory: N/A MB\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 234/234 [00:23<00:00, 10.10it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 14.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 | train loss: 0.0237 | test loss: 0.0385 | test acc: 9903/10000 (0.9903) | memory: N/A MB\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 234/234 [00:22<00:00, 10.19it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 14.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 | train loss: 0.0213 | test loss: 0.0410 | test acc: 9901/10000 (0.9901) | memory: N/A MB\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 234/234 [00:22<00:00, 10.29it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 14.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 | train loss: 0.0228 | test loss: 0.0451 | test acc: 9914/10000 (0.9914) | memory: N/A MB\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "deep_model = DeepNet().to(device)\n",
    "deep_optimizer = optim.Adam(deep_model.parameters(), lr=lr)\n",
    "\n",
    "print(\"> training deep model\\n\")\n",
    "for epoch in range(epochs):\n",
    "  train_losses = train(deep_model, train_loader, deep_optimizer, device)\n",
    "  test_losses = test(deep_model, test_loader, device)\n",
    "  print(f\"epoch: {epoch} | train loss: {train_losses['avg_loss']:.4f} | test loss: {test_losses['avg_loss']:.4f} | test acc: {test_losses['total_correct']}/{test_losses['num_datapoints']} ({test_losses['accuracy']:.4f}) | memory: {train_losses['memory']} MB\")\n",
    "  print('-' * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "import os\n",
    "from functools import partial\n",
    "\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import torch.multiprocessing as mp\n",
    "from torch.distributed.fsdp import (\n",
    "    FullyShardedDataParallel as FSDP,\n",
    ")\n",
    "from torch.distributed.fsdp.wrap import size_based_auto_wrap_policy\n",
    "\n",
    "# assert torch.cuda.is_available(), \"CUDA is not available. You must have CUDA enabled to use distributed training.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, distributed training in PyTorch requires a nontrivial amount of ceremony."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_free_addr():\n",
    "  return socket.gethostbyname_ex(socket.gethostname())[2][0]\n",
    "    \n",
    "def get_free_port(addr):\n",
    "  with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "    s.bind((addr, 0))\n",
    "    s.listen(1)\n",
    "    port = s.getsockname()[1]\n",
    "  return port\n",
    "\n",
    "def setup_distributed():\n",
    "  addr = get_free_addr()\n",
    "  port = get_free_port(addr)\n",
    "  os.environ['MASTER_ADDR'] = str(addr)\n",
    "  os.environ['MASTER_PORT'] = str(port)\n",
    "  if 'a100' in torch.cuda.get_device_name().lower(): # needed for training with A100's on Allen HPC\n",
    "    os.environ['NCCL_P2P_LVL'] = 'NVL'\n",
    "  dist.init_process_group(backend='nccl', rank=dist.get_rank(), world_size=dist.get_world_size())\n",
    "\n",
    "def cleanup_distributed():\n",
    "  dist.destroy_process_group()\n",
    "\n",
    "def is_master():\n",
    "  return dist.get_rank() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dist(model, train_loader, opt):\n",
    "  rank = dist.get_rank()\n",
    "  \n",
    "  total_loss = torch.zeros(1).to(rank)\n",
    "  num_batches = torch.zeros(1).to(rank)\n",
    "\n",
    "  model.train()\n",
    "  bar = tqdm(train_loader, total=len(train_loader)) if is_master() else train_loader\n",
    "  for (data, target) in bar:\n",
    "    data, target = data.to(rank), target.to(rank)\n",
    "    opt.zero_grad()\n",
    "    output = model(data)\n",
    "    loss = F.nll_loss(output, target, reduction='sum')\n",
    "    memory = torch.cuda.memory_allocated(rank) / 1e6 if torch.cuda.is_available() else 'N/A'\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    total_loss += loss\n",
    "    num_batches += 1\n",
    "  \n",
    "  dist.all_reduce(total_loss, op=dist.ReduceOp.SUM)\n",
    "  dist.all_reduce(num_batches, op=dist.ReduceOp.SUM)\n",
    "  avg_loss = total_loss.item() / (num_batches.item() * train_loader.batch_size)\n",
    "  return {\n",
    "    'avg_loss': avg_loss,\n",
    "    'memory': memory\n",
    "  }\n",
    "\n",
    "def test_dist(model, test_loader):\n",
    "  rank = dist.get_rank()\n",
    "\n",
    "  total_loss = torch.zeros(1).to(rank)\n",
    "  num_batches = torch.zeros(1).to(rank)\n",
    "  total_correct = torch.zeros(1).to(rank)\n",
    "  num_datapoints = torch.zeros(1).to(rank)\n",
    "\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    for (data, target) in test_loader:\n",
    "      data, target = data.to(rank), target.to(rank)\n",
    "      output = model(data)\n",
    "      total_loss += F.nll_loss(output, target, reduction='sum')\n",
    "      pred = output.argmax(dim=1, keepdim=True)\n",
    "      total_correct += pred.eq(target.view_as(pred)).sum()\n",
    "      num_datapoints += len(data)\n",
    "      num_batches += 1\n",
    "\n",
    "  dist.all_reduce(total_loss, op=dist.ReduceOp.SUM)\n",
    "  dist.all_reduce(num_batches, op=dist.ReduceOp.SUM)\n",
    "  dist.all_reduce(total_correct, op=dist.ReduceOp.SUM)\n",
    "  dist.all_reduce(num_datapoints, op=dist.ReduceOp.SUM)\n",
    "  avg_loss = total_loss.item() / (num_batches.item() * test_loader.batch_size)\n",
    "  accuracy = total_correct.item() / num_datapoints.item()\n",
    "  return {\n",
    "    'avg_loss': avg_loss,\n",
    "    'total_correct': total_correct.item(),\n",
    "    'num_datapoints': num_datapoints.item(),\n",
    "    'accuracy': accuracy\n",
    "  }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(rank, world_size):\n",
    "  train_batch_size = 256\n",
    "  test_batch_size = 1000\n",
    "  lr = 0.001\n",
    "  epochs = 10\n",
    "  min_num_params = 20000\n",
    "  auto_wrap_policy = partial(size_based_auto_wrap_policy, min_num_params=min_num_params)\n",
    "  \n",
    "  \n",
    "  setup_distributed()\n",
    "\n",
    "  _transform = transforms.Compose([\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize((0.1307,), (0.3081,))\n",
    "  ])\n",
    "  train_data = datasets.MNIST('../data', train=True, download=True, transform=_transform)\n",
    "  test_data = datasets.MNIST('../data', train=False, transform=_transform)\n",
    "\n",
    "  train_sampler = DistributedSampler(train_data, num_replicas=world_size, rank=rank)\n",
    "  test_sampler = DistributedSampler(test_data, num_replicas=world_size, rank=rank)\n",
    "\n",
    "  loader_kwargs = {\n",
    "      'num_workers': 2,\n",
    "      'pin_memory': True,\n",
    "      'shuffle': False,\n",
    "      'drop_last': True,\n",
    "      'persistent_workers': True # warning: on Allen HPC, disabling this massively slows down training\n",
    "  }\n",
    "\n",
    "  train_loader_dist = DataLoader(train_data, batch_size=train_batch_size, sampler=train_sampler, **loader_kwargs)\n",
    "  test_loader_dist = DataLoader(test_data, batch_size=test_batch_size, sampler=test_sampler, **loader_kwargs)\n",
    "\n",
    "  shallow_model = ShallowNet().to(rank)\n",
    "  shallow_model = FSDP(shallow_model, auto_wrap_policy=auto_wrap_policy)\n",
    "  shallow_optimizer = optim.Adam(shallow_model.parameters(), lr=lr)\n",
    "  if is_master():\n",
    "    print(\"> training shallow model\\n\")\n",
    "  for epoch in range(epochs):\n",
    "    train_sampler.set_epoch(epoch)\n",
    "    train_losses = train_dist(shallow_model, train_loader_dist, shallow_optimizer)\n",
    "\n",
    "    test_sampler.set_epoch(epoch)\n",
    "    test_losses = test_dist(shallow_model, test_loader_dist)\n",
    "    if is_master():\n",
    "      print(f\"epoch: {epoch} | train loss: {train_losses['avg_loss']:.4f} | test loss: {test_losses['avg_loss']:.4f} | test acc: {test_losses['total_correct']}/{test_losses['num_datapoints']} ({test_losses['accuracy']:.4f}) | memory: {train_losses['memory']} MB\")\n",
    "      print('-' * 100)\n",
    "    \n",
    "  deep_model = DeepNet().to(rank)\n",
    "  deep_model = FSDP(deep_model, auto_wrap_policy=auto_wrap_policy)\n",
    "  deep_optimizer = optim.Adam(deep_model.parameters(), lr=lr)\n",
    "  if is_master():\n",
    "    print(\"> training deep model\\n\")\n",
    "  for epoch in range(epochs):\n",
    "    train_sampler.set_epoch(epoch)\n",
    "    train_losses = train_dist(deep_model, train_loader_dist, deep_optimizer)\n",
    "\n",
    "    test_sampler.set_epoch(epoch)\n",
    "    test_losses = test_dist(deep_model, test_loader_dist)\n",
    "    if is_master():\n",
    "      print(f\"epoch: {epoch} | train loss: {train_losses['avg_loss']:.4f} | test loss: {test_losses['avg_loss']:.4f} | test acc: {test_losses['total_correct']}/{test_losses['num_datapoints']} ({test_losses['accuracy']:.4f}) | memory: {train_losses['memory']} MB\")\n",
    "      print('-' * 100)\n",
    "\n",
    "  cleanup_distributed()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp.spawn(main, args=(torch.cuda.device_count(),), nprocs=torch.cuda.device_count(), join=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mdist-mmidas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
