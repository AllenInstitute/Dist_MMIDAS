{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial is going to show you how to use PyTorch's fully-sharded data parallel (FSDP) implementation. We're going to train two models: (i) a shallow model, and (ii) a deep model. We will first train both models with local (no distributed/FSDP) training on a standard benchmark dataset (we chose MNIST for simplicity). Then, we will show the necessary code changes in order to use FSDP in PyTorch. In both local and distributed training, we log the time and memory, as well as losses. We will observe how FSDP training both decreases training time, and decreases memory consumption compared to local training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start off with local training. Please make sure you have installed the following before proceeding:\n",
    "- `torch` >= 2.0 with CUDA\n",
    "- `tqdm`\n",
    "\n",
    "This should be installed for you if you use the conda environment from `environment_312.yml` (instructions in the README). \n",
    "\n",
    "Additionally, the implementation of distributed training shown in this tutorial will require you to have at least 2 NVIDIA GPU available on your machine. You can check this with `nvidia-smi`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms # for datasets\n",
    "from tqdm import tqdm # for progress bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define our models. We will be using\n",
    "- a shallow model, ~1.2M params\n",
    "- a deep model, ~94M params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# params:\n",
      "  shallow: 1199882\n",
      "  deep: 94104234\n"
     ]
    }
   ],
   "source": [
    "class ShallowNet(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(ShallowNet, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "    self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "    self.dropout1 = nn.Dropout(0.25)\n",
    "    self.dropout2 = nn.Dropout(0.5)\n",
    "    self.fc1 = nn.Linear(9216, 128)\n",
    "    self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.conv1(x)\n",
    "    x = F.relu(x)\n",
    "    x = self.conv2(x)\n",
    "    x = F.relu(x)\n",
    "    x = F.max_pool2d(x, 2)\n",
    "    x = self.dropout1(x)\n",
    "    x = torch.flatten(x, 1)\n",
    "    x = self.fc1(x)\n",
    "    x = F.relu(x)\n",
    "    x = self.dropout2(x)\n",
    "    x = self.fc2(x)\n",
    "    output = F.log_softmax(x, dim=1)\n",
    "    return output\n",
    "  \n",
    "class DeepNet(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(DeepNet, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "    self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "    self.dropout1 = nn.Dropout(0.25)\n",
    "    self.dropout2 = nn.Dropout(0.5)\n",
    "    self.fc1 = nn.Linear(9216, 9000)\n",
    "    self.fc1a = nn.Linear(9000, 1000)\n",
    "    self.fc1b = nn.Linear(1000, 1000)\n",
    "    self.fc1c = nn.Linear(1000, 1000)\n",
    "    self.fc1d = nn.Linear(1000, 128)\n",
    "    self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.conv1(x)\n",
    "    x = F.relu(x)\n",
    "    x = self.conv2(x)\n",
    "    x = F.relu(x)\n",
    "    x = F.max_pool2d(x, 2)\n",
    "    x = self.dropout1(x)\n",
    "    x = torch.flatten(x, 1)\n",
    "    x = self.fc1(x)\n",
    "    x = F.relu(x)\n",
    "    x = self.dropout2(x)\n",
    "    x = self.fc1a(x)\n",
    "    x = self.fc1b(x)\n",
    "    x = self.fc1c(x)\n",
    "    x = self.fc1d(x)\n",
    "    x = self.fc2(x)\n",
    "    output = F.log_softmax(x, dim=1)\n",
    "    return output\n",
    "  \n",
    "# helper function\n",
    "def count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "  \n",
    "print(\"# params:\")\n",
    "print(f\"  shallow: {count_params(ShallowNet())}\")\n",
    "print(f\"  deep: {count_params(DeepNet())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the standard MNIST benchmark dataset. This choice is arbitrary and made for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset MNIST\n",
       "     Number of datapoints: 60000\n",
       "     Root location: ../data\n",
       "     Split: Train\n",
       "     StandardTransform\n",
       " Transform: Compose(\n",
       "                ToTensor()\n",
       "                Normalize(mean=(0.1307,), std=(0.3081,))\n",
       "            ),\n",
       " Dataset MNIST\n",
       "     Number of datapoints: 10000\n",
       "     Root location: ../data\n",
       "     Split: Test\n",
       "     StandardTransform\n",
       " Transform: Compose(\n",
       "                ToTensor()\n",
       "                Normalize(mean=(0.1307,), std=(0.3081,))\n",
       "            ))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_transform = transforms.Compose([\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize((0.1307,), (0.3081,))\n",
    "  ])\n",
    "\n",
    "train_data = datasets.MNIST('../data', train=True, download=True, transform=_transform)\n",
    "test_data = datasets.MNIST('../data', train=False, transform=_transform)\n",
    "\n",
    "train_data, test_data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we construct our dataloaders. Notice how we set `'persistent_workers': True`. If we set this to `False`, our training time massively slows down on Allen HPC.\n",
    "\n",
    "Additionally, one can optimize their per-epoch training time by tuning `num_workers`, but we do not go into that in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([tensor([[[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            ...,\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            ...,\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            ...,\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
       "  \n",
       "  \n",
       "          ...,\n",
       "  \n",
       "  \n",
       "          [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            ...,\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            ...,\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            ...,\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]]]),\n",
       "  tensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0, 9, 1,\n",
       "          1, 2, 4, 3, 2, 7, 3, 8, 6, 9, 0, 5, 6, 0, 7, 6, 1, 8, 7, 9, 3, 9, 8, 5,\n",
       "          9, 3, 3, 0, 7, 4, 9, 8, 0, 9, 4, 1, 4, 4, 6, 0, 4, 5, 6, 1, 0, 0, 1, 7,\n",
       "          1, 6, 3, 0, 2, 1, 1, 7, 9, 0, 2, 6, 7, 8, 3, 9, 0, 4, 6, 7, 4, 6, 8, 0,\n",
       "          7, 8, 3, 1, 5, 7, 1, 7, 1, 1, 6, 3, 0, 2, 9, 3, 1, 1, 0, 4, 9, 2, 0, 0,\n",
       "          2, 0, 2, 7, 1, 8, 6, 4, 1, 6, 3, 4, 5, 9, 1, 3, 3, 8, 5, 4, 7, 7, 4, 2,\n",
       "          8, 5, 8, 6, 7, 3, 4, 6, 1, 9, 9, 6, 0, 3, 7, 2, 8, 2, 9, 4, 4, 6, 4, 9,\n",
       "          7, 0, 9, 2, 9, 5, 1, 5, 9, 1, 2, 3, 2, 3, 5, 9, 1, 7, 6, 2, 8, 2, 2, 5,\n",
       "          0, 7, 4, 9, 7, 8, 3, 2, 1, 1, 8, 3, 6, 1, 0, 3, 1, 0, 0, 1, 7, 2, 7, 3,\n",
       "          0, 4, 6, 5, 2, 6, 4, 7, 1, 8, 9, 9, 3, 0, 7, 1, 0, 2, 0, 3, 5, 4, 6, 5,\n",
       "          8, 6, 3, 7, 5, 8, 0, 9, 1, 0, 3, 1, 2, 2, 3, 3])],\n",
       " [tensor([[[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            ...,\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            ...,\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            ...,\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
       "  \n",
       "  \n",
       "          ...,\n",
       "  \n",
       "  \n",
       "          [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            ...,\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            ...,\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            ...,\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "            [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]]]),\n",
       "  tensor([7, 2, 1, 0, 4, 1, 4, 9, 5, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4, 9, 6, 6, 5,\n",
       "          4, 0, 7, 4, 0, 1, 3, 1, 3, 4, 7, 2, 7, 1, 2, 1, 1, 7, 4, 2, 3, 5, 1, 2,\n",
       "          4, 4, 6, 3, 5, 5, 6, 0, 4, 1, 9, 5, 7, 8, 9, 3, 7, 4, 6, 4, 3, 0, 7, 0,\n",
       "          2, 9, 1, 7, 3, 2, 9, 7, 7, 6, 2, 7, 8, 4, 7, 3, 6, 1, 3, 6, 9, 3, 1, 4,\n",
       "          1, 7, 6, 9, 6, 0, 5, 4, 9, 9, 2, 1, 9, 4, 8, 7, 3, 9, 7, 4, 4, 4, 9, 2,\n",
       "          5, 4, 7, 6, 7, 9, 0, 5, 8, 5, 6, 6, 5, 7, 8, 1, 0, 1, 6, 4, 6, 7, 3, 1,\n",
       "          7, 1, 8, 2, 0, 2, 9, 9, 5, 5, 1, 5, 6, 0, 3, 4, 4, 6, 5, 4, 6, 5, 4, 5,\n",
       "          1, 4, 4, 7, 2, 3, 2, 7, 1, 8, 1, 8, 1, 8, 5, 0, 8, 9, 2, 5, 0, 1, 1, 1,\n",
       "          0, 9, 0, 3, 1, 6, 4, 2, 3, 6, 1, 1, 1, 3, 9, 5, 2, 9, 4, 5, 9, 3, 9, 0,\n",
       "          3, 6, 5, 5, 7, 2, 2, 7, 1, 2, 8, 4, 1, 7, 3, 3, 8, 8, 7, 9, 2, 2, 4, 1,\n",
       "          5, 9, 8, 7, 2, 3, 0, 4, 4, 2, 4, 1, 9, 5, 7, 7, 2, 8, 2, 6, 8, 5, 7, 7,\n",
       "          9, 1, 8, 1, 8, 0, 3, 0, 1, 9, 9, 4, 1, 8, 2, 1, 2, 9, 7, 5, 9, 2, 6, 4,\n",
       "          1, 5, 8, 2, 9, 2, 0, 4, 0, 0, 2, 8, 4, 7, 1, 2, 4, 0, 2, 7, 4, 3, 3, 0,\n",
       "          0, 3, 1, 9, 6, 5, 2, 5, 9, 2, 9, 3, 0, 4, 2, 0, 7, 1, 1, 2, 1, 5, 3, 3,\n",
       "          9, 7, 8, 6, 5, 6, 1, 3, 8, 1, 0, 5, 1, 3, 1, 5, 5, 6, 1, 8, 5, 1, 7, 9,\n",
       "          4, 6, 2, 2, 5, 0, 6, 5, 6, 3, 7, 2, 0, 8, 8, 5, 4, 1, 1, 4, 0, 3, 3, 7,\n",
       "          6, 1, 6, 2, 1, 9, 2, 8, 6, 1, 9, 5, 2, 5, 4, 4, 2, 8, 3, 8, 2, 4, 5, 0,\n",
       "          3, 1, 7, 7, 5, 7, 9, 7, 1, 9, 2, 1, 4, 2, 9, 2, 0, 4, 9, 1, 4, 8, 1, 8,\n",
       "          4, 5, 9, 8, 8, 3, 7, 6, 0, 0, 3, 0, 2, 6, 6, 4, 9, 3, 3, 3, 2, 3, 9, 1,\n",
       "          2, 6, 8, 0, 5, 6, 6, 6, 3, 8, 8, 2, 7, 5, 8, 9, 6, 1, 8, 4, 1, 2, 5, 9,\n",
       "          1, 9, 7, 5, 4, 0, 8, 9, 9, 1, 0, 5, 2, 3, 7, 8, 9, 4, 0, 6, 3, 9, 5, 2,\n",
       "          1, 3, 1, 3, 6, 5, 7, 4, 2, 2, 6, 3, 2, 6, 5, 4, 8, 9, 7, 1, 3, 0, 3, 8,\n",
       "          3, 1, 9, 3, 4, 4, 6, 4, 2, 1, 8, 2, 5, 4, 8, 8, 4, 0, 0, 2, 3, 2, 7, 7,\n",
       "          0, 8, 7, 4, 4, 7, 9, 6, 9, 0, 9, 8, 0, 4, 6, 0, 6, 3, 5, 4, 8, 3, 3, 9,\n",
       "          3, 3, 3, 7, 8, 0, 8, 2, 1, 7, 0, 6, 5, 4, 3, 8, 0, 9, 6, 3, 8, 0, 9, 9,\n",
       "          6, 8, 6, 8, 5, 7, 8, 6, 0, 2, 4, 0, 2, 2, 3, 1, 9, 7, 5, 1, 0, 8, 4, 6,\n",
       "          2, 6, 7, 9, 3, 2, 9, 8, 2, 2, 9, 2, 7, 3, 5, 9, 1, 8, 0, 2, 0, 5, 2, 1,\n",
       "          3, 7, 6, 7, 1, 2, 5, 8, 0, 3, 7, 2, 4, 0, 9, 1, 8, 6, 7, 7, 4, 3, 4, 9,\n",
       "          1, 9, 5, 1, 7, 3, 9, 7, 6, 9, 1, 3, 7, 8, 3, 3, 6, 7, 2, 8, 5, 8, 5, 1,\n",
       "          1, 4, 4, 3, 1, 0, 7, 7, 0, 7, 9, 4, 4, 8, 5, 5, 4, 0, 8, 2, 1, 0, 8, 4,\n",
       "          5, 0, 4, 0, 6, 1, 7, 3, 2, 6, 7, 2, 6, 9, 3, 1, 4, 6, 2, 5, 4, 2, 0, 6,\n",
       "          2, 1, 7, 3, 4, 1, 0, 5, 4, 3, 1, 1, 7, 4, 9, 9, 4, 8, 4, 0, 2, 4, 5, 1,\n",
       "          1, 6, 4, 7, 1, 9, 4, 2, 4, 1, 5, 5, 3, 8, 3, 1, 4, 5, 6, 8, 9, 4, 1, 5,\n",
       "          3, 8, 0, 3, 2, 5, 1, 2, 8, 3, 4, 4, 0, 8, 8, 3, 3, 1, 7, 3, 5, 9, 6, 3,\n",
       "          2, 6, 1, 3, 6, 0, 7, 2, 1, 7, 1, 4, 2, 4, 2, 1, 7, 9, 6, 1, 1, 2, 4, 8,\n",
       "          1, 7, 7, 4, 8, 0, 7, 3, 1, 3, 1, 0, 7, 7, 0, 3, 5, 5, 2, 7, 6, 6, 9, 2,\n",
       "          8, 3, 5, 2, 2, 5, 6, 0, 8, 2, 9, 2, 8, 8, 8, 8, 7, 4, 9, 3, 0, 6, 6, 3,\n",
       "          2, 1, 3, 2, 2, 9, 3, 0, 0, 5, 7, 8, 1, 4, 4, 6, 0, 2, 9, 1, 4, 7, 4, 7,\n",
       "          3, 9, 8, 8, 4, 7, 1, 2, 1, 2, 2, 3, 2, 3, 2, 3, 9, 1, 7, 4, 0, 3, 5, 5,\n",
       "          8, 6, 3, 2, 6, 7, 6, 6, 3, 2, 7, 8, 1, 1, 7, 5, 6, 4, 9, 5, 1, 3, 3, 4,\n",
       "          7, 8, 9, 1, 1, 6, 9, 1, 4, 4, 5, 4, 0, 6, 2, 2, 3, 1, 5, 1, 2, 0, 3, 8,\n",
       "          1, 2, 6, 7, 1, 6, 2, 3, 9, 0, 1, 2, 2, 0, 8, 9])])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch_size = 256\n",
    "test_batch_size = 1000\n",
    "\n",
    "\n",
    "loader_kwargs = {\n",
    "    'num_workers': 2,\n",
    "    'pin_memory': True,\n",
    "    'shuffle': False,\n",
    "    'drop_last': True,\n",
    "    'persistent_workers': True, # warning: on some SLURM clusters, disabling this massively slows down training\n",
    "}\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=train_batch_size, **loader_kwargs)\n",
    "test_loader = DataLoader(test_data, batch_size=test_batch_size, **loader_kwargs)\n",
    "\n",
    "next(iter(train_loader)), next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, out `train()` and `test()` functions. These functions will compute the per-sample loss for each batch of data. The `train()` function will additionally compute the memory allocated, if training on a NVIDIA GPU. Lastly, we use a `tqdm` progress bar during training to monitor our training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, opt, device):\n",
    "  total_loss = 0\n",
    "  num_batches = 0\n",
    "\n",
    "  model.train()\n",
    "  for (data, target) in tqdm(train_loader, total=len(train_loader)):\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    opt.zero_grad()\n",
    "    output = model(data)\n",
    "    loss = F.nll_loss(output, target, reduction='sum')\n",
    "    memory = torch.cuda.memory_allocated(device) / 1e6 if torch.cuda.is_available() else 'N/A' # memory in MB\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    total_loss += loss.item()\n",
    "    num_batches += 1\n",
    "\n",
    "  avg_loss = total_loss / (num_batches * train_loader.batch_size) # average loss per datapoint\n",
    "  return {\n",
    "    'avg_loss': avg_loss,\n",
    "    'memory': memory\n",
    "  }\n",
    "\n",
    "def test(model, test_loader, device):\n",
    "  total_loss = 0\n",
    "  num_batches = 0\n",
    "  total_correct = 0\n",
    "  num_datapoints = 0\n",
    "\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    for (data, target) in tqdm(test_loader, total=len(test_loader)):\n",
    "      data, target = data.to(device), target.to(device)\n",
    "      output = model(data)\n",
    "      total_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "      pred = output.argmax(dim=1, keepdim=True)\n",
    "      total_correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "      num_datapoints += len(data)\n",
    "      num_batches += 1\n",
    "  \n",
    "  avg_loss = total_loss / (num_batches * test_loader.batch_size) # average loss per datapoint\n",
    "  accuracy = total_correct / num_datapoints\n",
    "  return {\n",
    "    'avg_loss': avg_loss, \n",
    "    'total_correct': total_correct, \n",
    "    'num_datapoints': num_datapoints, \n",
    "    'accuracy': accuracy\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we train the shallow model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> training shallow model on cuda\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 234/234 [00:04<00:00, 53.89it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 11.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 | train loss: 0.2755 | test loss: 0.0539 | test acc: 9827/10000 (0.9827) | memory: 124.96 MB | time: 5.21s\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 234/234 [00:04<00:00, 56.46it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 11.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 | train loss: 0.0869 | test loss: 0.0427 | test acc: 9855/10000 (0.9855) | memory: 124.96 MB | time: 5.00s\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 234/234 [00:04<00:00, 56.69it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 11.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 | train loss: 0.0672 | test loss: 0.0326 | test acc: 9892/10000 (0.9892) | memory: 124.96 MB | time: 5.00s\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 234/234 [00:04<00:00, 56.31it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 11.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 | train loss: 0.0548 | test loss: 0.0365 | test acc: 9884/10000 (0.9884) | memory: 124.96 MB | time: 5.03s\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 234/234 [00:04<00:00, 56.31it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 11.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 | train loss: 0.0456 | test loss: 0.0324 | test acc: 9886/10000 (0.9886) | memory: 124.96 MB | time: 5.04s\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 234/234 [00:04<00:00, 56.18it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 11.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 | train loss: 0.0418 | test loss: 0.0338 | test acc: 9896/10000 (0.9896) | memory: 124.96 MB | time: 5.06s\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 234/234 [00:04<00:00, 56.56it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 11.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 | train loss: 0.0361 | test loss: 0.0344 | test acc: 9898/10000 (0.9898) | memory: 124.96 MB | time: 5.05s\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 234/234 [00:04<00:00, 55.89it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 11.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 | train loss: 0.0340 | test loss: 0.0316 | test acc: 9901/10000 (0.9901) | memory: 124.96 MB | time: 5.07s\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 234/234 [00:04<00:00, 56.13it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 11.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 | train loss: 0.0278 | test loss: 0.0395 | test acc: 9878/10000 (0.9878) | memory: 124.96 MB | time: 5.06s\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 234/234 [00:04<00:00, 55.64it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 11.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 | train loss: 0.0272 | test loss: 0.0308 | test acc: 9918/10000 (0.9918) | memory: 124.96 MB | time: 5.09s\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "lr = 0.001\n",
    "\n",
    "shallow_model = ShallowNet().to(device)\n",
    "shallow_optimizer = optim.Adam(shallow_model.parameters(), lr=lr)\n",
    "\n",
    "print(f\"> training shallow model on {device}\\n\")\n",
    "for epoch in range(epochs):\n",
    "  start_time = time.time()\n",
    "  train_losses = train(shallow_model, train_loader, shallow_optimizer, device)\n",
    "  test_losses = test(shallow_model, test_loader, device)\n",
    "  total_time = time.time() - start_time\n",
    "  print(f\"epoch: {epoch} | train loss: {train_losses['avg_loss']:.4f} | test loss: {test_losses['avg_loss']:.4f} | test acc: {test_losses['total_correct']}/{test_losses['num_datapoints']} ({test_losses['accuracy']:.4f}) | memory: {train_losses['memory']:.2f} MB | time: {total_time:.2f}s\")\n",
    "  print('-' * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we train the deep model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> training deep model on cuda\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 234/234 [00:05<00:00, 42.26it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 11.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 | train loss: 0.2750 | test loss: 0.0673 | test acc: 9809/10000 (0.9809) | memory: 1283.19 MB | time: 6.41s\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 234/234 [00:05<00:00, 42.91it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 11.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 | train loss: 0.0632 | test loss: 0.0515 | test acc: 9856/10000 (0.9856) | memory: 1283.19 MB | time: 6.33s\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 234/234 [00:05<00:00, 42.89it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 11.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 | train loss: 0.0426 | test loss: 0.0542 | test acc: 9856/10000 (0.9856) | memory: 1283.19 MB | time: 6.33s\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 234/234 [00:05<00:00, 42.72it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 14.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 | train loss: 0.0389 | test loss: 0.0664 | test acc: 9832/10000 (0.9832) | memory: 1283.19 MB | time: 6.18s\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 234/234 [00:05<00:00, 42.67it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 14.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 | train loss: 0.0316 | test loss: 0.0538 | test acc: 9878/10000 (0.9878) | memory: 1283.19 MB | time: 6.18s\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 234/234 [00:05<00:00, 42.69it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 14.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 | train loss: 0.0291 | test loss: 0.0595 | test acc: 9856/10000 (0.9856) | memory: 1283.19 MB | time: 6.19s\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 234/234 [00:05<00:00, 42.78it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 11.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 | train loss: 0.0288 | test loss: 0.0572 | test acc: 9875/10000 (0.9875) | memory: 1283.19 MB | time: 6.38s\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 234/234 [00:05<00:00, 42.79it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 14.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 | train loss: 0.2260 | test loss: 0.2105 | test acc: 9449/10000 (0.9449) | memory: 1283.19 MB | time: 6.18s\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 234/234 [00:05<00:00, 42.81it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 14.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 | train loss: 0.1073 | test loss: 0.0414 | test acc: 9884/10000 (0.9884) | memory: 1283.19 MB | time: 6.17s\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 234/234 [00:05<00:00, 42.78it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 14.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 | train loss: 0.0417 | test loss: 0.0514 | test acc: 9864/10000 (0.9864) | memory: 1283.19 MB | time: 6.17s\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "deep_model = DeepNet().to(device)\n",
    "deep_optimizer = optim.Adam(deep_model.parameters(), lr=lr)\n",
    "\n",
    "print(f\"> training deep model on {device}\\n\")\n",
    "for epoch in range(epochs):\n",
    "  start_time = time.time()\n",
    "  train_losses = train(deep_model, train_loader, deep_optimizer, device)\n",
    "  test_losses = test(deep_model, test_loader, device)\n",
    "  total_time = time.time() - start_time\n",
    "  print(f\"epoch: {epoch} | train loss: {train_losses['avg_loss']:.4f} | test loss: {test_losses['avg_loss']:.4f} | test acc: {test_losses['total_correct']}/{test_losses['num_datapoints']} ({test_losses['accuracy']:.4f}) | memory: {train_losses['memory']:.2f} MB | time: {total_time:.2f}s\")\n",
    "  print('-' * 100)\n",
    "\n",
    "del train_loader, test_loader # cleanup resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now move onto distributed training with FSDP. Note, this code currently is setup for training for training on multiple NVIDIA GPUs. Please make sure you have multiple NVIDIA GPUs available on your device (try `nvidia-smi` in the shell) before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Aug 21 11:03:56 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla V100-SXM2-32GB           On  |   00000000:61:00.0 Off |                    0 |\n",
      "| N/A   49C    P0             61W /  300W |    2676MiB /  32768MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2-32GB           On  |   00000000:62:00.0 Off |                    0 |\n",
      "| N/A   30C    P0             41W /  300W |       4MiB /  32768MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2-32GB           On  |   00000000:89:00.0 Off |                    0 |\n",
      "| N/A   29C    P0             42W /  300W |       4MiB /  32768MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2-32GB           On  |   00000000:8A:00.0 Off |                    0 |\n",
      "| N/A   30C    P0             43W /  300W |       4MiB /  32768MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A    174978      C   ...forge3/envs/mdist-mmidas/bin/python       2672MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we have our additional imports for FSDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "import os\n",
    "from functools import partial\n",
    "\n",
    "import torch.distributed as dist # for distributed communication\n",
    "from torch.utils.data.distributed import DistributedSampler # for distributed data across GPUs\n",
    "import torch.multiprocessing as mp # for spawning processes on each GPU\n",
    "from torch.distributed.fsdp import (\n",
    "    FullyShardedDataParallel as FSDP, # FSDP constructor for sharding model parameters\n",
    ")\n",
    "from torch.distributed.fsdp.wrap import size_based_auto_wrap_policy # for FSDP configuration\n",
    "\n",
    "assert torch.cuda.is_available(), \"CUDA is not available. You must have CUDA enabled to use distributed training.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we proceed, some basic vocabulary:\n",
    "\n",
    "`world_size` is the number of processes you will spawn for distributed training. This should be the GPUs on your device. So if you have 4 GPUs on your node/machine, you want to set `world_size` to 4.\n",
    "\n",
    "Then, `rank` is the ID of the current process. In order to use distributed rather than local training, we will spawn a separate Python process on each GPU using PyTorch's multiprocessing API. Then, we number processes by the GPU they are spawned on. For example, if you have 4 GPUs, one can think of their GPUs as `[GPU0, GPU1, GPU2, GPU3]`. Then, on each of `GPU0`, `GPU1`, `GPU2`, `GPU3`, one spawns a separate process. These processes are ID'd by their `rank`. So the process on GPU0 is called \"rank 0\", the process on GPU1 is called \"rank 1\", etc.\n",
    "\n",
    "We will use \"world size\", \"`world_size`\", and \"number of GPUs\" interchangeably in this tutorial. Additionally, we will use \"`rank`\", \"rank\", \"process\" and \"spawned process\" interchangeably.\n",
    "\n",
    "After initializing PyTorch's distributed backend using `torch.distributed.init_process_group()`, one can always compute the rank of the current process via `torch.distributed.get_rank()`. Similarly, one can compute the world size (that is, the number of processes spawned, which should be the number of GPUs available on one's device) via `torch.distributed.get_world_size()`. The world size is the same across ranks. \n",
    "\n",
    "Finally, before the program exits, one should call `torch.distributed.destroy_process_group()` in order to cleanup any remaining resources. This is simply what PyTorch requires us to go through in order to use distributed training with `torch.distributed`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, distributed training in PyTorch requires a nontrivial amount of ceremony. The next cell contains some utility functions to initialize PyTorch's distributed backend. There is more than one way to initialize the distributed backend for distributed training in PyTorch. We chose this implementation for simplicity. The most important parts are: setting necessary environmental variables (`MASTER_ADDR` and `MASTER_PORT`) before initializing a process group for distributed training, calling `torch.distributed.init_process_group()` to initialize distributed training after spawning multiple processes, and calling `torch.distributed.destroy_process_group()` after distributed training to cleanup resources.\n",
    "\n",
    "Notice, we have some additional code to handle if our GPU is an NVIDIA A100. When testing FSDP on some SLURM HPC clusters, we found we needed to set the environment flag `NCCL_P2P_LVL` to value `NVL` in order for PyTorch's distributed backend to not timeout when using NVIDIA A100's. When using A100's on other machines and cloud GPU providers (such as Lambda Labs), setting the `NCCL_P2P_LVL` environmental variable was not necessary. Additionally, setting this environmental variable was not necessary when using other NVIDIA GPUs (V100, TitanXP, etc.) on SLURM HPC clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility functions to initialize distributed training\n",
    "\n",
    "def get_free_addr():\n",
    "  return socket.gethostbyname_ex(socket.gethostname())[2][0]\n",
    "    \n",
    "def get_free_port(addr):\n",
    "  with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "    s.bind((addr, 0))\n",
    "    s.listen(1)\n",
    "    port = s.getsockname()[1]\n",
    "  return port\n",
    "\n",
    "# initialize PyTorch's distributed backend\n",
    "def setup_distributed(rank, world_size, addr, port):\n",
    "  os.environ['MASTER_ADDR'] = str(addr) # address that ranks will use to communicate\n",
    "  os.environ['MASTER_PORT'] = str(port) # port that ranks will use to communicate\n",
    "  if 'a100' in torch.cuda.get_device_name().lower(): # needed for training with A100's on some SLURM clusters\n",
    "    os.environ['NCCL_P2P_LVL'] = 'NVL'\n",
    "  dist.init_process_group(backend='nccl', rank=rank, world_size=world_size)\n",
    "\n",
    "# cleanup PyTorch's distributed backend\n",
    "def cleanup_distributed():\n",
    "  dist.destroy_process_group()\n",
    "\n",
    "# we only want to print on the master rank (rank 0) to avoid duplicate logging. This is a convention commonly followed\n",
    "# when doing distributed training to reduce logging clutter\n",
    "def is_master():\n",
    "  return dist.get_rank() == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then modify our `train()` and `test()` functions. Our modified `train()` function is called `train_dist()`. Similarly, `test()` is now `test_dist()`.\n",
    "\n",
    "We now store our losses as `torch.Tensor` objects on each rank. This is so we can sum our losses across ranks using `torch.distributed.all_reduce()` to get the average losses-per-sample across all samples across all ranks. The `all_reduce()` function is simply an element-wise sum of tensors. For example, suppose you have 4 ranks: rank 0, rank 1, rank 2, rank 3. And a tensor `t` on each rank. If `t` is `torch.Tensor([2])` on rank 0, `torch.Tensor([3])` on rank 1, `torch.Tensor([9])` on rank 2, and `torch.tensor([1])` on rank 3, then after calling `torch.distributed.all_reduce(t)`, then `t` will be `torch.Tensor([2 + 3 + 1 + 9 = 15])` on rank 0, `torch.Tensor([15])` on rank 1, and so on.\n",
    "\n",
    "Additionally, we now only display the progress bar on rank 0 to reduce clutter.\n",
    "\n",
    "You may notice this is not the most concise or succinct code possible for distributed training. Our goal here was readability. You are more than welcome to alter this implementation when doing your distributed training as you see fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dist(model, train_loader, opt):\n",
    "  rank = dist.get_rank() # new\n",
    "  \n",
    "  total_loss = torch.zeros(1).to(rank) # updated\n",
    "  num_batches = torch.zeros(1).to(rank) # updated\n",
    "\n",
    "  model.train()\n",
    "  bar = tqdm(train_loader, total=len(train_loader)) if is_master() else train_loader # updated: we only want a progress bar on the master rank\n",
    "  for (data, target) in bar: # updated\n",
    "    data, target = data.to(rank), target.to(rank) # updated\n",
    "    opt.zero_grad()\n",
    "    output = model(data)\n",
    "    loss = F.nll_loss(output, target, reduction='sum')\n",
    "    memory = torch.cuda.memory_allocated(rank) / 1e6 if torch.cuda.is_available() else 'N/A'\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    total_loss += loss\n",
    "    num_batches += 1\n",
    "  \n",
    "  dist.all_reduce(total_loss, op=dist.ReduceOp.SUM) # new\n",
    "  dist.all_reduce(num_batches, op=dist.ReduceOp.SUM) # new\n",
    "  avg_loss = total_loss.item() / (num_batches.item() * train_loader.batch_size) # updated\n",
    "  return {\n",
    "    'avg_loss': avg_loss,\n",
    "    'memory': memory\n",
    "  }\n",
    "\n",
    "def test_dist(model, test_loader):\n",
    "  rank = dist.get_rank() # new\n",
    "\n",
    "  total_loss = torch.zeros(1).to(rank) # updated\n",
    "  num_batches = torch.zeros(1).to(rank) # updated\n",
    "  total_correct = torch.zeros(1).to(rank) # updated\n",
    "  num_datapoints = torch.zeros(1).to(rank) # updated\n",
    "\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    bar = tqdm(test_loader, total=len(test_loader)) if is_master() else test_loader\n",
    "    for (data, target) in bar: # updated: we only want a progress bar on the master rank\n",
    "      data, target = data.to(rank), target.to(rank)\n",
    "      output = model(data)\n",
    "      total_loss += F.nll_loss(output, target, reduction='sum')\n",
    "      pred = output.argmax(dim=1, keepdim=True)\n",
    "      total_correct += pred.eq(target.view_as(pred)).sum()\n",
    "      num_datapoints += len(data)\n",
    "      num_batches += 1\n",
    "\n",
    "  dist.all_reduce(total_loss, op=dist.ReduceOp.SUM)\n",
    "  dist.all_reduce(num_batches, op=dist.ReduceOp.SUM)\n",
    "  dist.all_reduce(total_correct, op=dist.ReduceOp.SUM)\n",
    "  dist.all_reduce(num_datapoints, op=dist.ReduceOp.SUM)\n",
    "  avg_loss = total_loss.item() / (num_batches.item() * test_loader.batch_size)\n",
    "  accuracy = total_correct.item() / num_datapoints.item()\n",
    "  return {\n",
    "    'avg_loss': avg_loss,\n",
    "    'total_correct': total_correct.item(),\n",
    "    'num_datapoints': num_datapoints.item(),\n",
    "    'accuracy': accuracy\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have now added a `main()` function. We now have to encapsulate all of our training logic in a single function in order to do distributed training in PyTorch. This is the entry-point for each spawned process/rank.\n",
    "\n",
    "The most significant changes in `main()` from the training code earlier in the notebook are:\n",
    "- initializing and cleaning up the distributed process group (line 13, line 75)\n",
    "- constructing a `DistributedSampler` object to distribute data across ranks for data-parallelism (lines 24-25, lines 36-37)\n",
    "- sharding the model across devices with the `FSDP` constructor (line 42, line 60)\n",
    "\n",
    "Using a `DistributedSampler` allows us to incorporate data-parallelism in our training loop, in addition to sharding model parameters across ranks. Data-parallelism speeds up training, similar to using `DistributedDataParallelism`. Sharding model parameters across ranks decreases memory consumption.\n",
    "\n",
    "Pay attention to lines 9 and 10. We must tell PyTorch how to shard our model parameters across ranks via a \"wrapping policy.\" In addition to this wrapping policy, the `FullyShardedDataParallelism` implementation in PyTorch has *many* hyperparameters you can tune to optimize the performance of the FSDP algorithm for your model's particular architecture and training loop. In addition, `FullyShardedDataParallelism` in PyTorch allows you to combine FSDP with other large-scale model training techniques, such as mixed-precision training, multi-dimensional parallelism (so combining with training strategies such as *tensor parallelism* or *pipeline parallelism*), and JIT compilation with `torch.compile()`. We do not go into any of these advanced techniques in this tutorial. If you have questions on using these techniques, you're more than welcome to reach out to me (email at the end of this notebook).\n",
    "\n",
    "Additionally, there are some other small code changes in this block, not mentioned in this explanation. Please pay close attention to anything labeled `new` or `updated`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(rank, world_size, addr, port):\n",
    "  # training config, same as before\n",
    "  train_batch_size = 256\n",
    "  test_batch_size = 1000\n",
    "  lr = 0.001\n",
    "  epochs = 10\n",
    "  \n",
    "  # new: some config for FSDP\n",
    "  min_num_params = 20000\n",
    "  auto_wrap_policy = partial(size_based_auto_wrap_policy, min_num_params=min_num_params)\n",
    "  \n",
    "  \n",
    "  setup_distributed(rank, world_size, addr, port) # new: initialize PyTorch's distributed backend\n",
    "\n",
    "  torch.cuda.set_device(rank) # new: set the device to the current rank\n",
    "\n",
    "  _transform = transforms.Compose([\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize((0.1307,), (0.3081,))\n",
    "  ])\n",
    "  train_data = datasets.MNIST('../data', train=True, download=True, transform=_transform)\n",
    "  test_data = datasets.MNIST('../data', train=False, transform=_transform)\n",
    "\n",
    "  train_sampler = DistributedSampler(train_data, num_replicas=world_size, rank=rank) # new: use DistributedSampler to distribute data\n",
    "                                                                                     #      across all ranks\n",
    "  test_sampler = DistributedSampler(test_data, num_replicas=world_size, rank=rank)\n",
    "\n",
    "  loader_kwargs = {\n",
    "      'num_workers': 2,\n",
    "      'pin_memory': True,\n",
    "      'shuffle': False,\n",
    "      'drop_last': True,\n",
    "      'persistent_workers': True # warning: on Allen HPC, disabling this massively slows down training\n",
    "  }\n",
    "\n",
    "  train_loader_dist = DataLoader(train_data, batch_size=train_batch_size, sampler=train_sampler, **loader_kwargs) # updated: make sure to pass in the sampler\n",
    "  test_loader_dist = DataLoader(test_data, batch_size=test_batch_size, sampler=test_sampler, **loader_kwargs)\n",
    "\n",
    "  # First, we train the shallow model\n",
    "\n",
    "  shallow_model = ShallowNet().to(rank)\n",
    "  shallow_model = FSDP(shallow_model, auto_wrap_policy=auto_wrap_policy) # new: wrap the model with FSDP. This will shard the model's parameters across ranks during training\n",
    "  shallow_optimizer = optim.Adam(shallow_model.parameters(), lr=lr) # make sure to construct the optimizer AFTER wrapping the model with FSDP, as we want to update the \n",
    "                                                                    # parameters of the sharded model, not  original model\n",
    "  if is_master():\n",
    "    print(\"> training shallow model\\n\") # updated: only print on the master rank\n",
    "  for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    train_sampler.set_epoch(epoch) # new: we must ensure each rank works on a different partition of the same batch of data\n",
    "    train_losses = train_dist(shallow_model, train_loader_dist, shallow_optimizer)\n",
    "\n",
    "    test_sampler.set_epoch(epoch) # new: we must ensure each rank works on a different partition of the same batch of data\n",
    "    test_losses = test_dist(shallow_model, test_loader_dist)\n",
    "    total_time = time.time() - start_time\n",
    "    if is_master(): # updated: only print on the master rank\n",
    "      print(f\"epoch: {epoch} | train loss: {train_losses['avg_loss']:.4f} | test loss: {test_losses['avg_loss']:.4f} | test acc: {test_losses['total_correct']}/{test_losses['num_datapoints']} ({test_losses['accuracy']:.4f}) | memory: {train_losses['memory']:.2f} MB | time: {total_time:.2f}s\")\n",
    "      print('-' * 100)\n",
    "    \n",
    "  # Next, we train the deep model. Most of what applied to the shallow model applies here, too.\n",
    "\n",
    "  deep_model = DeepNet().to(rank)\n",
    "  deep_model = FSDP(deep_model, auto_wrap_policy=auto_wrap_policy)\n",
    "  deep_optimizer = optim.Adam(deep_model.parameters(), lr=lr)\n",
    "  if is_master():\n",
    "    print(\"> training deep model\\n\")\n",
    "  for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    train_sampler.set_epoch(epoch)\n",
    "    train_losses = train_dist(deep_model, train_loader_dist, deep_optimizer)\n",
    "\n",
    "    test_sampler.set_epoch(epoch)\n",
    "    test_losses = test_dist(deep_model, test_loader_dist)\n",
    "    total_time = time.time() - start_time\n",
    "    if is_master():\n",
    "      print(f\"epoch: {epoch} | train loss: {train_losses['avg_loss']:.4f} | test loss: {test_losses['avg_loss']:.4f} | test acc: {test_losses['total_correct']}/{test_losses['num_datapoints']} ({test_losses['accuracy']:.4f}) | memory: {train_losses['memory']:.2f} MB | time: {total_time:.2f}s\")\n",
    "      print('-' * 100)\n",
    "\n",
    "  cleanup_distributed() # new: cleanup PyTorch's distributed backend to release resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we spawn a separate process for each GPU using PyTorch's multiprocessing API.\n",
    "\n",
    "We define our master address and master port once, and pass the same address and port to each process as an argument to `main()`.\n",
    "\n",
    "We call `fsdp_main()` from the `fsdp_tutorial.py` (notice the extension) Python script rather than the `main()` function in this notebook as a workaround to use `multiprocesing` in a Jupyter notebook. We must have the function we want to attach processes to with `multiprocessing.spawn()` be defined a separate file, which we then import into this notebook. If we try calling `mp.spawn(main, ...)`, you will notice that we get an error. Worry not, the code in `fsdp_tutorial.py` is exactly the same as in this notebook.\n",
    "\n",
    "Additionally, on some SLURM HPC clusters, the first epoch of distributed training may be very slow compared to the other epochs. This is due to to overhead of spawning multiple workers in the `Dataloader`. This will not affect the training time of the rest of your epochs, though. The rest of your epochs should train much more quickly than local training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> training shallow model\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58/58 [00:17<00:00,  3.40it/s]\n",
      "100%|██████████| 2/2 [00:11<00:00,  5.68s/it]\n",
      "  2%|▏         | 1/58 [00:00<00:06,  8.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 | train loss: 0.5351 | test loss: 0.1191 | test acc: 7712.0/8000.0 (0.9640) | memory: 115.16 MB | time: 28.41s\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58/58 [00:01<00:00, 40.93it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 11.80it/s]\n",
      "  0%|          | 0/58 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 | train loss: 0.1504 | test loss: 0.0589 | test acc: 7846.0/8000.0 (0.9808) | memory: 115.16 MB | time: 1.59s\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58/58 [00:01<00:00, 36.50it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  9.75it/s]\n",
      "  2%|▏         | 1/58 [00:00<00:08,  6.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 | train loss: 0.0989 | test loss: 0.0475 | test acc: 7877.0/8000.0 (0.9846) | memory: 115.16 MB | time: 1.80s\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58/58 [00:01<00:00, 38.82it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 11.42it/s]\n",
      "  2%|▏         | 1/58 [00:00<00:07,  7.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 | train loss: 0.0775 | test loss: 0.0388 | test acc: 7884.0/8000.0 (0.9855) | memory: 115.16 MB | time: 1.67s\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58/58 [00:01<00:00, 42.71it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 11.38it/s]\n",
      "  0%|          | 0/58 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 | train loss: 0.0671 | test loss: 0.0363 | test acc: 7901.0/8000.0 (0.9876) | memory: 115.16 MB | time: 1.55s\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58/58 [00:01<00:00, 39.54it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  9.81it/s]\n",
      "  2%|▏         | 1/58 [00:00<00:07,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 | train loss: 0.0574 | test loss: 0.0374 | test acc: 7906.0/8000.0 (0.9882) | memory: 115.16 MB | time: 1.68s\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58/58 [00:01<00:00, 42.19it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  9.88it/s]\n",
      "  0%|          | 0/58 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 | train loss: 0.0521 | test loss: 0.0279 | test acc: 7920.0/8000.0 (0.9900) | memory: 115.16 MB | time: 1.58s\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58/58 [00:01<00:00, 33.41it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 12.76it/s]\n",
      "  0%|          | 0/58 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 | train loss: 0.0459 | test loss: 0.0319 | test acc: 7916.0/8000.0 (0.9895) | memory: 115.16 MB | time: 1.91s\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58/58 [00:01<00:00, 41.43it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 12.57it/s]\n",
      "  2%|▏         | 1/58 [00:00<00:08,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 | train loss: 0.0427 | test loss: 0.0299 | test acc: 7922.0/8000.0 (0.9902) | memory: 115.16 MB | time: 1.60s\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58/58 [00:01<00:00, 39.11it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 11.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 | train loss: 0.0397 | test loss: 0.0293 | test acc: 7928.0/8000.0 (0.9910) | memory: 115.16 MB | time: 1.69s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "> training deep model\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58/58 [00:02<00:00, 24.78it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 10.12it/s]\n",
      "  0%|          | 0/58 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 | train loss: 0.9412 | test loss: 0.0637 | test acc: 7848.0/8000.0 (0.9810) | memory: 422.25 MB | time: 2.56s\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58/58 [00:02<00:00, 25.36it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 10.21it/s]\n",
      "  0%|          | 0/58 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 | train loss: 0.0641 | test loss: 0.0376 | test acc: 7902.0/8000.0 (0.9878) | memory: 422.25 MB | time: 2.50s\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58/58 [00:02<00:00, 25.13it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 10.46it/s]\n",
      "  0%|          | 0/58 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 | train loss: 0.0345 | test loss: 0.0372 | test acc: 7901.0/8000.0 (0.9876) | memory: 422.25 MB | time: 2.52s\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58/58 [00:02<00:00, 25.00it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 10.39it/s]\n",
      "  0%|          | 0/58 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 | train loss: 0.0226 | test loss: 0.0331 | test acc: 7906.0/8000.0 (0.9882) | memory: 422.25 MB | time: 2.53s\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58/58 [00:02<00:00, 25.02it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  9.88it/s]\n",
      "  0%|          | 0/58 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 | train loss: 0.0149 | test loss: 0.0368 | test acc: 7915.0/8000.0 (0.9894) | memory: 422.25 MB | time: 2.54s\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58/58 [00:02<00:00, 24.99it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 10.20it/s]\n",
      "  0%|          | 0/58 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 | train loss: 0.0137 | test loss: 0.0329 | test acc: 7927.0/8000.0 (0.9909) | memory: 422.25 MB | time: 2.54s\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58/58 [00:02<00:00, 25.17it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 10.46it/s]\n",
      "  0%|          | 0/58 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 | train loss: 0.0106 | test loss: 0.0357 | test acc: 7923.0/8000.0 (0.9904) | memory: 422.25 MB | time: 2.52s\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58/58 [00:02<00:00, 25.20it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 10.62it/s]\n",
      "  0%|          | 0/58 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 | train loss: 0.0101 | test loss: 0.0355 | test acc: 7921.0/8000.0 (0.9901) | memory: 422.25 MB | time: 2.51s\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58/58 [00:02<00:00, 25.16it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 10.08it/s]\n",
      "  0%|          | 0/58 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 | train loss: 0.0072 | test loss: 0.0360 | test acc: 7925.0/8000.0 (0.9906) | memory: 422.25 MB | time: 2.52s\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58/58 [00:02<00:00, 25.05it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 10.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 | train loss: 0.0090 | test loss: 0.0355 | test acc: 7923.0/8000.0 (0.9904) | memory: 422.25 MB | time: 2.53s\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from fsdp_tutorial import main as fsdp_main\n",
    "\n",
    "world_size = torch.cuda.device_count() # new: number of GPUs\n",
    "addr = get_free_addr()\n",
    "port = get_free_port(addr)\n",
    "mp.spawn(fsdp_main, args=(world_size, addr, port), nprocs=world_size, join=True) # new: spawn a process on each GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we're done! Hopefully you've now learned how to use FSDP training with PyTorch, including how to setup PyTorch's distributed backend, and how you can shard your model's parameters across GPU devices with PyTorch's FSDP API.\n",
    "\n",
    "If you want to learn more, PyTorch's FSDP tutorials are pretty good:\n",
    "- https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html\n",
    "- https://pytorch.org/tutorials/intermediate/FSDP_adavnced_tutorial.html\n",
    "\n",
    "The FSDP whitepaper is also pretty good. It includes a lot of implementation details you can use to speedup your training:\n",
    "- https://arxiv.org/pdf/2304.11277\n",
    "\n",
    "The documentation for FSDP can answer a lot of your questions:\n",
    "- https://pytorch.org/docs/stable/fsdp.html\n",
    "\n",
    "For anything the above resources can't answer, the FSDP source is publicly available:\n",
    "- https://github.com/pytorch/pytorch/blob/main/torch/distributed/fsdp/fully_sharded_data_parallel.py/\n",
    "\n",
    "Note, that in PyTorch 2.4, the PyTorch team is updating FSDP to \"FSDP 2.0\". So everything shown in this tutorial (as well as all the above resources) are for \"FSDP 1.0\". As of August 2024, the FSDP 2.0 API is experimental. However, if you want to learn more about it, you may find these resources useful:\n",
    "- https://github.com/pytorch/torchtitan/blob/main/docs/fsdp.md (FSDP 2.0 overview)\n",
    "- https://github.com/pytorch/pytorch/blob/main/torch/distributed/_composable/fully_shard.py (FSDP 2.0 source)\n",
    "\n",
    "If you have any questions, please email me at `hilal.mufti@alleninstitute.org` or `hmufti@cs.washington.edu`. I'm more than happy to help with any distributed training questions you may still have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "This tutorial expands on the ideas and code in this official PyTorch tutorial: https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mdist-mmidas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
